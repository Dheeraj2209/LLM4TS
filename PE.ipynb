{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "#second commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1511 entries, 0 to 1510\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    1511 non-null   object \n",
      " 1   Open    1511 non-null   float64\n",
      " 2   High    1511 non-null   float64\n",
      " 3   Low     1511 non-null   float64\n",
      " 4   Close   1511 non-null   float64\n",
      " 5   Volume  1511 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 71.0+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   # a library for data analysis and manipulation\n",
    "import torch          # qa library for ML\n",
    "import transformers   # a library for NLP techniques from Hugging Face.\n",
    "\n",
    "#what is hugging face?\n",
    "#It is an open source community that has built a library called transformers that has all pre trined models\n",
    "\n",
    "#read the proper CSV file\n",
    "df=pd.read_csv(\"Microsoft_Stock.csv\")\n",
    "\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-01 16:00:00</th>\n",
       "      <td>40.60</td>\n",
       "      <td>40.76</td>\n",
       "      <td>40.31</td>\n",
       "      <td>40.72</td>\n",
       "      <td>36865322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-02 16:00:00</th>\n",
       "      <td>40.66</td>\n",
       "      <td>40.74</td>\n",
       "      <td>40.12</td>\n",
       "      <td>40.29</td>\n",
       "      <td>37487476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-06 16:00:00</th>\n",
       "      <td>40.34</td>\n",
       "      <td>41.78</td>\n",
       "      <td>40.18</td>\n",
       "      <td>41.55</td>\n",
       "      <td>39223692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-07 16:00:00</th>\n",
       "      <td>41.61</td>\n",
       "      <td>41.91</td>\n",
       "      <td>41.31</td>\n",
       "      <td>41.53</td>\n",
       "      <td>28809375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-08 16:00:00</th>\n",
       "      <td>41.48</td>\n",
       "      <td>41.69</td>\n",
       "      <td>41.04</td>\n",
       "      <td>41.42</td>\n",
       "      <td>24753438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Open   High    Low  Close    Volume\n",
       "Date                                                     \n",
       "2015-04-01 16:00:00  40.60  40.76  40.31  40.72  36865322\n",
       "2015-04-02 16:00:00  40.66  40.74  40.12  40.29  37487476\n",
       "2015-04-06 16:00:00  40.34  41.78  40.18  41.55  39223692\n",
       "2015-04-07 16:00:00  41.61  41.91  41.31  41.53  28809375\n",
       "2015-04-08 16:00:00  41.48  41.69  41.04  41.42  24753438"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the date column to proper date and time format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "#Now set the Date column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensure all values are numeric or replace it with NaN\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Handle missing values by filling them with the mean of the column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "#display the columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tensor shape: torch.Size([755, 10, 5])\n",
      "Sample from Final Temporal Tensor:\n",
      " tensor([[[   2,    1,    4,    2, 2015],\n",
      "         [   3,    2,    4,    2, 2015],\n",
      "         [   0,    6,    4,    2, 2015],\n",
      "         [   1,    7,    4,    2, 2015],\n",
      "         [   2,    8,    4,    2, 2015],\n",
      "         [   3,    9,    4,    2, 2015],\n",
      "         [   4,   10,    4,    2, 2015],\n",
      "         [   0,   13,    4,    2, 2015],\n",
      "         [   1,   14,    4,    2, 2015],\n",
      "         [   2,   15,    4,    2, 2015]],\n",
      "\n",
      "        [[   3,   16,    4,    2, 2015],\n",
      "         [   4,   17,    4,    2, 2015],\n",
      "         [   0,   20,    4,    2, 2015],\n",
      "         [   1,   21,    4,    2, 2015],\n",
      "         [   2,   22,    4,    2, 2015],\n",
      "         [   3,   23,    4,    2, 2015],\n",
      "         [   4,   24,    4,    2, 2015],\n",
      "         [   0,   27,    4,    2, 2015],\n",
      "         [   1,   28,    4,    2, 2015],\n",
      "         [   2,   29,    4,    2, 2015]],\n",
      "\n",
      "        [[   3,   30,    4,    2, 2015],\n",
      "         [   4,    1,    5,    2, 2015],\n",
      "         [   0,    4,    5,    2, 2015],\n",
      "         [   1,    5,    5,    2, 2015],\n",
      "         [   2,    6,    5,    2, 2015],\n",
      "         [   3,    7,    5,    2, 2015],\n",
      "         [   4,    8,    5,    2, 2015],\n",
      "         [   0,   11,    5,    2, 2015],\n",
      "         [   1,   12,    5,    2, 2015],\n",
      "         [   2,   13,    5,    2, 2015]],\n",
      "\n",
      "        [[   3,   14,    5,    2, 2015],\n",
      "         [   4,   15,    5,    2, 2015],\n",
      "         [   0,   18,    5,    2, 2015],\n",
      "         [   1,   19,    5,    2, 2015],\n",
      "         [   2,   20,    5,    2, 2015],\n",
      "         [   3,   21,    5,    2, 2015],\n",
      "         [   4,   22,    5,    2, 2015],\n",
      "         [   1,   26,    5,    2, 2015],\n",
      "         [   2,   27,    5,    2, 2015],\n",
      "         [   3,   28,    5,    2, 2015]],\n",
      "\n",
      "        [[   4,   29,    5,    2, 2015],\n",
      "         [   0,    1,    6,    2, 2015],\n",
      "         [   1,    2,    6,    2, 2015],\n",
      "         [   2,    3,    6,    2, 2015],\n",
      "         [   3,    4,    6,    2, 2015],\n",
      "         [   4,    5,    6,    2, 2015],\n",
      "         [   0,    8,    6,    2, 2015],\n",
      "         [   1,    9,    6,    2, 2015],\n",
      "         [   2,   10,    6,    2, 2015],\n",
      "         [   3,   11,    6,    2, 2015]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract temporal attributes\n",
    "day_of_week = torch.tensor(df.index.dayofweek, dtype=torch.int32)  # 0 = Monday, 6 = Sunday\n",
    "day_of_month = torch.tensor(df.index.day, dtype=torch.int32)       # 1 to 31\n",
    "year = torch.tensor(df.index.year, dtype=torch.int32)              # Year\n",
    "month = torch.tensor(df.index.month, dtype=torch.int32)            # Month (1 to 12)\n",
    "quarter = torch.tensor(df.index.quarter, dtype=torch.int32)        # Quarter (1 to 4)\n",
    "\n",
    "# Set parameters\n",
    "num_labels = 5  # Number of labels (repetition factor)\n",
    "patch_length = 10  # Length of each patch\n",
    "\n",
    "# Step 3: Repeat each temporal attribute tensor\n",
    "day_of_week_repeated = day_of_week.repeat(num_labels)\n",
    "day_of_month_repeated = day_of_month.repeat(num_labels)\n",
    "year_repeated = year.repeat(num_labels)\n",
    "month_repeated = month.repeat(num_labels)\n",
    "quarter_repeated = quarter.repeat(num_labels)\n",
    "\n",
    "# Step 4: Define the patching function\n",
    "def patching(x, patch_length):\n",
    "    num_elements = x.shape[0]\n",
    "    num_patches = num_elements // patch_length\n",
    "    x = x[:num_patches * patch_length]  # Ensure the tensor length is a multiple of patch_length\n",
    "    x = x.view(num_patches, patch_length)\n",
    "    return x\n",
    "\n",
    "# Step 5: Apply patching to each repeated tensor\n",
    "day_of_week_patched = patching(day_of_week_repeated, patch_length)\n",
    "day_of_month_patched = patching(day_of_month_repeated, patch_length)\n",
    "year_patched = patching(year_repeated, patch_length)\n",
    "month_patched = patching(month_repeated, patch_length)\n",
    "quarter_patched = patching(quarter_repeated, patch_length)\n",
    "\n",
    "# Step 6: Stack the patched tensors to create the final 3D tensor\n",
    "final_tensor = torch.stack(\n",
    "    (day_of_week_patched, day_of_month_patched, month_patched, quarter_patched, year_patched), \n",
    "    dim=-1\n",
    ")\n",
    "\n",
    "# Verify final shape\n",
    "num_patches, patch_len, num_temporal_attributes = final_tensor.shape\n",
    "print(f\"Final tensor shape: {final_tensor.shape}\")  # Expected: (num_labels * num_data_points / patch_length, patch_length, num_temporal_attributes)\n",
    "\n",
    "# Display first few entries to verify\n",
    "print(\"Sample from Final Temporal Tensor:\\n\", final_tensor[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1511 entries, 2015-04-01 16:00:00 to 2021-03-31 16:00:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Open    1511 non-null   float64\n",
      " 1   High    1511 non-null   float64\n",
      " 2   Low     1511 non-null   float64\n",
      " 3   Close   1511 non-null   float64\n",
      " 4   Volume  1511 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 70.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "\n",
    "#open : the price at which the stock has opened\n",
    "#high : the highest price of a stock\n",
    "#volume: total no of share brought in that stock\n",
    "\n",
    "# all of these are for that particular day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1511, 5)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1781, -1.1794, -1.1788, -1.1764,  0.4678],\n",
      "        [-1.1770, -1.1798, -1.1822, -1.1839,  0.5114],\n",
      "        [-1.1826, -1.1616, -1.1811, -1.1617,  0.6332],\n",
      "        ...,\n",
      "        [ 2.2791,  2.2370,  2.2435,  2.2542, -0.3488],\n",
      "        [ 2.2251,  2.1856,  2.2296,  2.1944, -0.3793],\n",
      "        [ 2.2142,  2.2771,  2.2526,  2.2635,  0.9419]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InstanceNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        std = x.std(dim=0, keepdim=True)\n",
    "        return (x - mean) / std\n",
    "\n",
    "# Convert data to PyTorch tensor\n",
    "time_series = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "# Apply instance normalization\n",
    "instance_norm = InstanceNormalization()\n",
    "normalized_series = instance_norm(time_series)\n",
    "\n",
    "print(normalized_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1511, 5])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Applying channel independence to convert multiple univariate time series data into univariate time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel-Independent Tensor: tensor([[-1.1781],\n",
      "        [-1.1770],\n",
      "        [-1.1826],\n",
      "        ...,\n",
      "        [-0.3488],\n",
      "        [-0.3793],\n",
      "        [ 0.9419]])\n",
      "Shape of channel-independent tensor: torch.Size([7555, 1])\n"
     ]
    }
   ],
   "source": [
    "# Channel-independence\n",
    "def channel_independence(x):\n",
    "    batch_size, num_features = x.shape            #number of elements in total, number of columns\n",
    "    x=x.T\n",
    "    x = x.reshape(batch_size * num_features, 1)   # Combine time steps and features into a single dimension\n",
    "    return x\n",
    "\n",
    "#easier for trend analysis and season analysis in the time series.\n",
    "ci_series = channel_independence(normalized_series)\n",
    "\n",
    "print(\"Channel-Independent Tensor:\", ci_series)\n",
    "print(\"Shape of channel-independent tensor:\", ci_series.shape)  # Should be (1511*5, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Patching to reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched Tensor: tensor([[-1.1781, -1.1770, -1.1826,  ..., -1.1640, -1.1569, -1.1576],\n",
      "        [-1.1543, -1.1592, -1.1581,  ..., -1.0611, -1.0514, -1.0348],\n",
      "        [-1.0352, -1.0373, -1.0410,  ..., -1.0555, -1.0678, -1.0442],\n",
      "        ...,\n",
      "        [-0.5621, -1.0137, -0.9568,  ...,  0.4384,  0.0021, -0.2707],\n",
      "        [ 0.6556,  0.5347, -0.3414,  ...,  0.3556,  0.2022, -0.0317],\n",
      "        [-0.0204, -0.5294, -0.2916,  ..., -0.0050,  0.1010, -0.3212]])\n",
      "Shape of patched tensor: torch.Size([755, 10])\n"
     ]
    }
   ],
   "source": [
    "def patching(x, patch_length):\n",
    "    num_elements = x.shape[0]\n",
    "    num_patches = num_elements // patch_length\n",
    "    x = x[:num_patches * patch_length]  # Ensure the tensor length is a multiple of patch_length\n",
    "    x = x.view(num_patches, patch_length)\n",
    "    return x\n",
    "\n",
    "patch_length = 10  # Example patch length\n",
    "patched_series = patching(ci_series, patch_length)\n",
    "\n",
    "print(\"Patched Tensor:\", patched_series)\n",
    "print(\"Shape of patched tensor:\", patched_series.shape)  # Should be (num_patches, patch_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Three Encodings for Patched Time-Series Data\n",
    "\n",
    "4.1 First part of it will be to employ the new CNN encoding layer for the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a class for the convolution token layer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvToken(nn.Module):\n",
    "    def __init__(self, patch_length, embedding_dim):\n",
    "        super(ConvToken, self).__init__()\n",
    "        # Convolutional layer with kernel size equal to patch length, producing a single embedding per patch\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=embedding_dim, kernel_size=patch_length, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add a channel dimension for Conv1d and apply convolution\n",
    "        x = x.unsqueeze(1)  # Shape: (num_patches, 1, patch_length)\n",
    "        x = self.conv(x)  # Output Shape: (num_patches, embedding_dim, 1)\n",
    "        return x.squeeze(-1)  # Final Shape: (num_patches, embedding_dim)\n",
    "\n",
    "\n",
    "# Define positional encoding class\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create an embedding matrix to learn positional embeddings\n",
    "        # Each position (from 0 to num_patches-1) gets a corresponding embedding of size embedding_dim\n",
    "        self.embedding = nn.Embedding(num_patches, embedding_dim)\n",
    "\n",
    "    # Forward pass method for positional encoding\n",
    "    def forward(self, x):\n",
    "        # Generate positional indices (0, 1, 2, ..., num_patches-1)\n",
    "        # These indices represent the position of each patch\n",
    "        positions = torch.arange(x.size(0), device=x.device).unsqueeze(0)\n",
    "\n",
    "        # Use the positional indices to get the corresponding positional embeddings\n",
    "        # Now we have an embedding for each position in the sequence\n",
    "        return self.embedding(positions)\n",
    "    \n",
    "\n",
    "# # Example parameters\n",
    "# patch_length = 10  # Length of each patch\n",
    "# embedding_dim = 128  # Dimension of the embedding space\n",
    "# num_patches = patched_series.shape[0]  # Number of patches in the sequence\n",
    "\n",
    "# # Initialize the ConvToken and PositionalEncoding layers\n",
    "# conv_token = ConvToken(patch_length, embedding_dim)\n",
    "# pos_encoding = PositionalEncoding(num_patches, embedding_dim)\n",
    "\n",
    "# # Generate token and positional embeddings\n",
    "# token_embeddings = conv_token(patched_series)  # Shape: (num_patches, embedding_dim)\n",
    "# pos_embeddings = pos_encoding(token_embeddings)  # Shape: (num_patches, embedding_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Creating the temporal encoding layer to address the challenge LLMs face in processing multi-scale temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Embedding Tensor: tensor([[-4.1726, -2.2333,  2.1285,  ...,  4.6208,  4.6860, -0.5629],\n",
      "        [-4.3357, -2.5969,  0.7983,  ...,  3.1878,  2.0729, -2.5815],\n",
      "        [-3.8686, -2.5627,  0.8053,  ...,  2.0667,  3.9806, -3.2453],\n",
      "        ...,\n",
      "        [ 1.7099, -0.0471, -0.6279,  ...,  1.3870,  0.9682,  0.7003],\n",
      "        [ 1.1803, -0.0080,  0.2090,  ...,  1.5019,  0.8307,  0.0377],\n",
      "        [-3.3308, -2.9931, -1.0281,  ...,  1.3491, -0.3906,  0.5315]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Shape of Temporal Embedding Tensor: torch.Size([755, 128])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, embedding_dim, max_values):\n",
    "            \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_patches (int): Number of patches in the dataset.\n",
    "            embedding_dim (int): Dimension of each embedding vector.\n",
    "            max_values (dict): Dictionary specifying max values for each temporal attribute.\n",
    "                               For example: {'sec': 59, 'min': 59, 'hour': 23, 'day': 31, 'month': 12, 'year': 2100}\n",
    "        \"\"\"\n",
    "        super(TemporalEncoding, self).__init__()\n",
    "        \n",
    "        # Create trainable embeddings for each temporal attribute\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            attr: nn.Embedding(max_val + 1, embedding_dim)\n",
    "            for attr, max_val in max_values.items()\n",
    "        })\n",
    "\n",
    "        # Pooling method: Select the first timestamp in each patch\n",
    "        self.pooling = nn.AdaptiveMaxPool1d(1)  # Can replace with other pooling methods if needed\n",
    "\n",
    "    def forward(self, temporal_attributes_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            temporal_attributes_tensor (torch.Tensor): Tensor of shape (num_patches, num_timestamps, num_attributes)\n",
    "                                                      containing temporal attributes.\n",
    "                                                      Example: [day_of_week, day_of_month, year, month, quarter]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Temporal embedding tensor of shape (num_patches, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Extract embeddings for each temporal attribute and sum them (Level 1 Aggregation)\n",
    "        temporal_embedding = 0\n",
    "        for i, (attr, embedding_layer) in enumerate(self.embeddings.items()):\n",
    "            attr_values = temporal_attributes_tensor[:, :, i]  # Shape: (num_patches, num_timestamps)\n",
    "            temporal_embedding += embedding_layer(attr_values)  # Shape: (num_patches, num_timestamps, embedding_dim)\n",
    "        \n",
    "        # Level 2 Aggregation: Pool across timestamps within each patch\n",
    "        # Assuming we are using \"select first\" as the pooling method\n",
    "        temporal_embedding = temporal_embedding[:, 0, :]  # Select the first timestamp in each patch\n",
    "        # Shape after pooling: (num_patches, embedding_dim)\n",
    "\n",
    "        return temporal_embedding\n",
    "\n",
    "# Example usage\n",
    "# Suppose temporal_attributes_tensor is a tensor with shape (num_patches, num_timestamps, num_attributes)\n",
    "# containing [day_of_week, day_of_month, year, month, quarter]\n",
    "\n",
    "# Define max values for each temporal attribute\n",
    "max_values = {\n",
    "    'day_of_week': 6,   # 0 = Monday, 6 = Sunday\n",
    "    'day_of_month': 31, # 1 to 31\n",
    "    'month': 12,        # 1 to 12\n",
    "    'quarter': 4,       # 1 to 4\n",
    "    'year': 2025 # Assuming years in range 2000-2100\n",
    "}\n",
    "\n",
    "embedding_dim = 128  # Example embedding dimension\n",
    "num_patches = patched_series.size(0)  # Example number of patches\n",
    "num_timestamps = 10  # Example number of timestamps per patch\n",
    "num_attributes = len(max_values)\n",
    "\n",
    "# Example temporal attributes tensor\n",
    "temporal_attributes_tensor = final_tensor #shape (num_patches, num_timestamps, num_attributes)\n",
    "\n",
    "# # Initialize and apply the TemporalEncoding layer\n",
    "temporal_encoder = TemporalEncoding(num_patches, embedding_dim, max_values)\n",
    "temporal_embedding = temporal_encoder(temporal_attributes_tensor)\n",
    "\n",
    "print(\"Temporal Embedding Tensor:\", temporal_embedding)\n",
    "print(\"Shape of Temporal Embedding Tensor:\", temporal_embedding.shape)  # Expected shape: (num_patches,Â embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Now preserving the parameters for the pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import copy\n",
    "# from huggingface_hub import hf_hub_download\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # Set up the access token\n",
    "# HUGGING_FACE_API_KEY = \"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\"\n",
    "\n",
    "# # Define model ID and file names\n",
    "# model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# filenames = [\n",
    "#     \"LICENSE\",\n",
    "#     \"README.md\",\n",
    "#     \"USE_POLICY.md\",\n",
    "#     \"config.json\",\n",
    "#     \"generation_config.json\",\n",
    "#     \"model-00001-of-00004.safetensors\",\n",
    "#     \"model-00002-of-00004.safetensors\",\n",
    "#     \"model-00003-of-00004.safetensors\",\n",
    "#     \"model-00004-of-00004.safetensors\",\n",
    "#     \"model.safetensors.index.json\",\n",
    "#     \"special_tokens_map.json\",\n",
    "#     \"tokenizer.json\",\n",
    "#     \"tokenizer_config.json\"\n",
    "# ]\n",
    "\n",
    "# # Download necessary model files (this is only needed if not automatically handled)\n",
    "# for filename in filenames:\n",
    "#     download_model_path = hf_hub_download(\n",
    "#         repo_id=model_id,\n",
    "#         filename=filename,\n",
    "#         token=HUGGING_FACE_API_KEY\n",
    "#     )\n",
    "#     print(\"Downloaded:\", download_model_path)\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=HUGGING_FACE_API_KEY)\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=HUGGING_FACE_API_KEY)\n",
    "\n",
    "# # Set padding token if not already set\n",
    "# tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "# pretrained_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# # Custom PretrainedLLM model\n",
    "# # class PretrainedLLM(nn.Module):\n",
    "# #     def __init__(self, pretrained_model, num_layers, embedding_dim, prediction_dim):\n",
    "# #         super(PretrainedLLM, self).__init__()\n",
    "# #         # Freeze most parameters of the pre-trained model\n",
    "# #         for param in pretrained_model.parameters():\n",
    "# #             param.requires_grad = False\n",
    "\n",
    "# #         # Extract specific layers from the pre-trained model\n",
    "# #         # Extract specific layers from the pre-trained model for LLaMA\n",
    "# #         self.transformer_blocks = nn.ModuleList([copy.deepcopy(pretrained_model.model.layers[i]) for i in range(num_layers)])\n",
    "\n",
    "\n",
    "# #         # Linear output layer\n",
    "# #         self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "# #     def forward(self, embeddings):\n",
    "# #         # Pass embeddings through selected Transformer blocks\n",
    "# #         for transformer_block in self.transformer_blocks:\n",
    "# #             embeddings = transformer_block(embeddings)[0]  # Only take hidden states\n",
    "\n",
    "# #         # Linear output layer\n",
    "# #         output = self.linear(embeddings)\n",
    "# #         return output\n",
    "\n",
    "# class PretrainedLLM(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_layers, embedding_dim, prediction_dim):\n",
    "#         super(PretrainedLLM, self).__init__()\n",
    "        \n",
    "#         # Freeze pretrained model parameters\n",
    "#         for param in pretrained_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "            \n",
    "#         # Get LLaMA's hidden dimension\n",
    "#         self.llama_hidden_dim = pretrained_model.config.hidden_size  # Usually 4096 for LLaMA\n",
    "        \n",
    "#         # Project input embeddings to LLaMA's hidden dimension\n",
    "#         self.input_projection = nn.Linear(embedding_dim, self.llama_hidden_dim)\n",
    "        \n",
    "#         # Extract specific layers from the pre-trained model\n",
    "#         self.transformer_blocks = nn.ModuleList([\n",
    "#             copy.deepcopy(pretrained_model.model.layers[i]) \n",
    "#             for i in range(num_layers)\n",
    "#         ])\n",
    "        \n",
    "#         # Project from LLaMA's hidden dimension to prediction dimension\n",
    "#         self.output_projection = nn.Linear(self.llama_hidden_dim, prediction_dim)\n",
    "        \n",
    "#         # Layer normalization for input and output\n",
    "#         self.input_norm = nn.LayerNorm(self.llama_hidden_dim)\n",
    "#         self.output_norm = nn.LayerNorm(prediction_dim)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#         # Project embeddings to LLaMA's hidden dimension\n",
    "#         x = self.input_projection(embeddings)\n",
    "#         x = self.input_norm(x)\n",
    "        \n",
    "#         # Prepare attention mask (assuming no padding)\n",
    "#         batch_size = x.size(0)\n",
    "#         attention_mask = torch.ones(batch_size, x.size(1), device=x.device)\n",
    "        \n",
    "#         # Pass through transformer blocks\n",
    "#         for block in self.transformer_blocks:\n",
    "#             # Create the expected input structure for LLaMA layers\n",
    "#             transformer_outputs = block(\n",
    "#                 hidden_states=x,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 position_ids=None,\n",
    "#                 past_key_value=None,\n",
    "#                 output_attentions=False,\n",
    "#                 use_cache=False,\n",
    "#             )\n",
    "#             x = transformer_outputs[0]\n",
    "        \n",
    "#         # Project to prediction dimension\n",
    "#         x = self.output_projection(x)\n",
    "#         output = self.output_norm(x)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "# # Example parameters\n",
    "# num_layers = 12           # Number of Transformer blocks to use\n",
    "# embedding_dim = 128       # Dimension of the embeddings\n",
    "# num_patches = 755         # Number of patches\n",
    "# prediction_dim = 128      # Dimension of prediction (output)\n",
    "\n",
    "# # Initialize PretrainedLLM\n",
    "# pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, prediction_dim)\n",
    "\n",
    "# # Example input embeddings\n",
    "# embeddings = torch.randn(32,num_patches, embedding_dim)\n",
    "\n",
    "# # Forward pass through the custom model\n",
    "# output = pretrained_llm(embeddings)\n",
    " \n",
    "# # Example ground truth shifted patched data\n",
    "# p_shifted = torch.randn(32,num_patches, prediction_dim)\n",
    "\n",
    "# # Calculate Mean Squared Error (MSE) loss\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss = loss_fn(output, p_shifted)\n",
    "\n",
    "# print(\"MSE Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.226328134536743\n",
      "Epoch 2/10, Loss: 2.2223784923553467\n",
      "Epoch 3/10, Loss: 1.621949553489685\n",
      "Epoch 4/10, Loss: 1.3589149713516235\n",
      "Epoch 5/10, Loss: 1.23776376247406\n",
      "Epoch 6/10, Loss: 1.1786693334579468\n",
      "Epoch 7/10, Loss: 1.145270824432373\n",
      "Epoch 8/10, Loss: 1.1239924430847168\n",
      "Epoch 9/10, Loss: 1.1119211912155151\n",
      "Epoch 10/10, Loss: 1.105441927909851\n",
      "Final MSE Loss: 1.1034008264541626\n"
     ]
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "        # Freeze most parameters of the pre-trained model\n",
    "        for param in pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Extract the necessary layers from the pre-trained model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Linear output layer\n",
    "        self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "        # Apply linear output layer\n",
    "        output = self.linear(embeddings)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Example parameters\n",
    "num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "embedding_dim = 768  # Dimension of the embeddings\n",
    "num_patches = 512  # Number of patches\n",
    "prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# Initialize PretrainedLLM\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# Example input: embeddings after temporal encoding\n",
    "embeddings = torch.randn(1, num_patches, embedding_dim)  # Add a batch dimension\n",
    "\n",
    "# Forward pass\n",
    "output = pretrained_llm(embeddings)\n",
    "\n",
    "# Example ground truth shifted patched data\n",
    "p_shifted = torch.randn(1, num_patches, prediction_dim)  # Add a batch dimension\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(output, p_shifted)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())'''\n",
    "\n",
    "# Code with MSE loss of 70\n",
    "\n",
    "'''import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "        # Extract the necessary layers from the pre-trained model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Additional layers for fine-tuning\n",
    "        self.dropout = nn.Dropout(0.1)  # Add dropout for regularization\n",
    "        self.linear1 = nn.Linear(embedding_dim, prediction_dim)  # Additional linear layer\n",
    "        self.activation = nn.ReLU()  # Additional activation function\n",
    "        self.linear2 = nn.Linear(prediction_dim, prediction_dim)  # Additional linear layer\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "        # Apply additional layers\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        embeddings = self.linear1(embeddings)\n",
    "        embeddings = self.activation(embeddings)\n",
    "        embeddings = self.linear2(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Example parameters\n",
    "num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "embedding_dim = 768  # Dimension of the embeddings\n",
    "num_patches = 512  # Number of patches\n",
    "prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# Initialize PretrainedLLM\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# Example input: embeddings after temporal encoding\n",
    "embeddings = torch.randn(1, num_patches, embedding_dim)  # Add a batch dimension\n",
    "\n",
    "# Forward pass\n",
    "output = pretrained_llm(embeddings)\n",
    "\n",
    "# Example ground truth shifted patched data\n",
    "p_shifted = torch.randn(1, num_patches, prediction_dim)  # Add a batch dimension\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(output, p_shifted)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())'''\n",
    "\n",
    "#Most updated code :---\n",
    "'''import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, get_scheduler\n",
    "\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "        # Freeze most parameters of the pre-trained model\n",
    "        for param in pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Extract the necessary layers from the pre-trained model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Linear output layer\n",
    "        self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "        # Apply linear output layer\n",
    "        output = self.linear(embeddings)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Example parameters\n",
    "num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "embedding_dim = 768  # Dimension of the embeddings\n",
    "num_patches = 512  # Number of patches\n",
    "prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# Initialize PretrainedLLM\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "# PretrainedLLM class that builds on top of a pretrained language model (like GPT-2)\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim, num_additional_layers=1, hidden_dim=512, dropout_rate=0.1, activation='relu'):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "\n",
    "        # Use a pre-trained model (like GPT-2) and extract its transformer blocks\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Additional fully connected layers after the transformer blocks\n",
    "        # These are added to enhance the output of the pre-trained model\n",
    "        self.additional_layers = nn.ModuleList()\n",
    "        for _ in range(num_additional_layers):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(embedding_dim, hidden_dim),  # Linear layer to project embeddings to hidden_dim\n",
    "                nn.Dropout(dropout_rate),  # Dropout for regularization (prevent overfitting)\n",
    "                nn.ReLU() if activation == 'relu' else nn.LeakyReLU(),  # Non-linear activation function\n",
    "                nn.Linear(hidden_dim, embedding_dim)  # Linear layer to project back to embedding_dim\n",
    "            )\n",
    "            self.additional_layers.append(layer)\n",
    "\n",
    "        # Final linear layer to produce the prediction output\n",
    "        self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Pass the input embeddings through the transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embeddings = transformer_block(embeddings)[0]  # Transformer block output, keeping only the embeddings\n",
    "\n",
    "        # Apply additional layers (fully connected layers) to the embeddings\n",
    "        for layer in self.additional_layers:\n",
    "            embeddings = layer(embeddings)\n",
    "\n",
    "        # Final output through the linear layer to predict the desired output\n",
    "        output = self.linear(embeddings)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "# Load a pre-trained GPT-2 model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the number of transformer layers (12 in GPT-2), embedding dimension (768), and number of patches (512)\n",
    "num_layers = 12\n",
    "embedding_dim = 768\n",
    "num_patches = 512\n",
    "prediction_dim = 768  # The dimensionality of the output prediction\n",
    "\n",
    "# Additional settings for the fully connected layers added to the model\n",
    "num_additional_layers = 2  # Adding 2 extra layers for better predictions\n",
    "hidden_dim = 512  # The size of the hidden layers\n",
    "dropout_rate = 0.1  # Dropout rate for regularization\n",
    "activation = 'relu'  # Activation function to use (ReLU)\n",
    "\n",
    "# Initialize the PretrainedLLM model\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim,\n",
    "                               num_additional_layers=num_additional_layers, hidden_dim=hidden_dim,\n",
    "                               dropout_rate=dropout_rate, activation=activation)\n",
    "\n",
    "# Example input: Random tensor representing embeddings after temporal encoding (shape: [batch_size, num_patches, embedding_dim])\n",
    "embeddings = torch.randn(1, num_patches, embedding_dim)  # Adding a batch dimension (size 1)\n",
    "\n",
    "# Example ground truth: Shifted patched data for prediction task (shape: [batch_size, num_patches, prediction_dim])\n",
    "p_shifted = torch.randn(1, num_patches, prediction_dim)  # Similar batch size and patch dimension as input\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 16  # Size of the mini-batches for training\n",
    "num_epochs = 10  # Number of training iterations (epochs)\n",
    "learning_rate = 5e-5  # Learning rate for the optimizer\n",
    "\n",
    "# Define optimizer (AdamW is often used for transformer models) and a learning rate scheduler\n",
    "optimizer = AdamW(pretrained_llm.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=num_epochs // 10, num_training_steps=num_epochs)\n",
    "\n",
    "# Loss function: Mean Squared Error (MSE) for regression-like tasks\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop to train the model over several epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: Get model predictions from input embeddings\n",
    "    output = pretrained_llm(embeddings)\n",
    "\n",
    "    # Calculate the loss by comparing the model output with the true data (p_shifted)\n",
    "    loss = loss_fn(output, p_shifted)\n",
    "\n",
    "    # Backpropagation: Zero the gradients, compute gradients, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print loss for each epoch to track training progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Final evaluation of the model after training\n",
    "output = pretrained_llm(embeddings)\n",
    "final_loss = loss_fn(output, p_shifted)\n",
    "print(\"Final MSE Loss:\", final_loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 forcasting finetuning\n",
    "+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 1.9983710050582886\n",
      "Epoch 1/10, Loss: 1.9896456003189087\n",
      "Epoch 2/10, Loss: 1.9847488403320312\n",
      "Epoch 3/10, Loss: 1.9775680303573608\n",
      "Epoch 4/10, Loss: 1.975267767906189\n",
      "Epoch 5/10, Loss: 1.962453842163086\n",
      "Epoch 6/10, Loss: 1.9645633697509766\n",
      "Epoch 7/10, Loss: 1.9530686140060425\n",
      "Epoch 8/10, Loss: 1.9464088678359985\n",
      "Epoch 9/10, Loss: 1.937732219696045\n",
      "Epoch 10/10, Loss: 1.928073763847351\n",
      "Final MSE Loss: 1.928073763847351\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the reversible instance normalization (RevIN) class\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features, affine=True):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features  # Number of features in the input data\n",
    "        # InstanceNorm1d applies normalization to the features across a batch\n",
    "        self.norm = nn.InstanceNorm1d(num_features, affine=affine)\n",
    "        # Learnable scale and shift parameters to adjust the normalized data\n",
    "        self.affine_scale = nn.Parameter(torch.ones(1, num_features, 1))\n",
    "        self.affine_shift = nn.Parameter(torch.zeros(1, num_features, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transpose the input so that it's in the correct shape for normalization\n",
    "        x_transposed = x.transpose(1, 2)  # (batch_size, num_features, seq_len)\n",
    "        # Apply instance normalization to standardize the data\n",
    "        normed = self.norm(x_transposed)  # (batch_size, num_features, seq_len)\n",
    "        # Apply scaling and shifting to normalized data\n",
    "        normed = normed * self.affine_scale + self.affine_shift\n",
    "        # Transpose the data back to the original shape\n",
    "        normed = normed.transpose(1, 2)  # (batch_size, seq_len, num_features)\n",
    "\n",
    "        return normed\n",
    "\n",
    "# Main Forecasting Model that includes the RevIN and GPT-2 based transformer\n",
    "class ForecastingModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "        super(ForecastingModel, self).__init__()\n",
    "        # Reversible instance normalization (RevIN) to normalize input data\n",
    "        self.revin = RevIN(embedding_dim)\n",
    "\n",
    "        # Select the transformer blocks (layers) from the pretrained GPT-2 model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Additional layers for prediction: a dropout, a linear layer, and an activation function (ReLU)\n",
    "        self.dropout = nn.Dropout(0.1)  # Dropout for regularization (prevents overfitting)\n",
    "        self.linear1 = nn.Linear(embedding_dim, embedding_dim)  # First linear transformation\n",
    "        self.activation = nn.ReLU()  # Activation function to introduce non-linearity\n",
    "        self.linear2 = nn.Linear(embedding_dim, prediction_dim)  # Second linear transformation for prediction\n",
    "\n",
    "        # Output layer to transform the prediction back to the original embedding dimension\n",
    "        self.output_linear = nn.Linear(prediction_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply normalization using RevIN\n",
    "        x = self.revin(x)\n",
    "\n",
    "        # Pass the normalized data through each transformer block (pretrained GPT-2 layers)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)[0]  # Extract the output from each block\n",
    "\n",
    "        # Apply the additional prediction layers (dropout -> linear -> activation -> linear)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        # Final transformation and denormalize the output using RevIN\n",
    "        x = self.output_linear(x)\n",
    "        x = self.revin(x)  # Reverse the normalization to get data back in its original form\n",
    "\n",
    "        return x\n",
    "\n",
    "# Load the pretrained GPT-2 model and tokenizer for later use\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Define the model parameters\n",
    "num_layers = 12  # Use 12 transformer layers from GPT-2\n",
    "embedding_dim = 768  # The dimension of the input embeddings\n",
    "num_patches = 512  # The number of time steps (sequence length)\n",
    "prediction_dim = 768  # Prediction output dimension (same as input dimension)\n",
    "\n",
    "# Initialize the forecasting model with the GPT-2 transformer\n",
    "forecasting_model = ForecastingModel(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# Example input: random data to simulate temporal embeddings (e.g., stock price data)\n",
    "embeddings = torch.randn(1, num_patches, embedding_dim)  # Random input with batch size 1\n",
    "\n",
    "# Perform a forward pass through the model to get predictions\n",
    "output = forecasting_model(embeddings)\n",
    "\n",
    "# Example of ground truth future data (what we want to predict)\n",
    "future_data = torch.randn(1, num_patches, embedding_dim)  # Random target data for loss calculation\n",
    "\n",
    "# Define Mean Squared Error (MSE) as the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# Calculate the loss between the model output (predictions) and future data\n",
    "loss = loss_fn(output, future_data)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())\n",
    "\n",
    "# Fine-tuning the model (training the model to minimize the loss)\n",
    "optimizer = torch.optim.Adam(forecasting_model.parameters(), lr=1e-4)  # Adam optimizer for gradient descent\n",
    "num_epochs = 10  # Number of times to train on the entire dataset\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    forecasting_model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Zero out gradients from the previous step\n",
    "    output = forecasting_model(embeddings)  # Forward pass through the model\n",
    "    loss = loss_fn(output, future_data)  # Calculate loss\n",
    "    loss.backward()  # Backpropagate the loss to compute gradients\n",
    "    optimizer.step()  # Update the model weights using gradients\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")  # Output training progress\n",
    "\n",
    "# Final loss after training\n",
    "print(\"Final MSE Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
