{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "#second commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1511 entries, 0 to 1510\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    1511 non-null   object \n",
      " 1   Open    1511 non-null   float64\n",
      " 2   High    1511 non-null   float64\n",
      " 3   Low     1511 non-null   float64\n",
      " 4   Close   1511 non-null   float64\n",
      " 5   Volume  1511 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 71.0+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   # a library for data analysis and manipulation\n",
    "import torch          # qa library for ML\n",
    "import transformers   # a library for NLP techniques from Hugging Face.\n",
    "\n",
    "#what is hugging face?\n",
    "#It is an open source community that has built a library called transformers that has all pre trined models\n",
    "\n",
    "#read the proper CSV file\n",
    "df=pd.read_csv(\"Microsoft_Stock.csv\")\n",
    "\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-01 16:00:00</th>\n",
       "      <td>40.60</td>\n",
       "      <td>40.76</td>\n",
       "      <td>40.31</td>\n",
       "      <td>40.72</td>\n",
       "      <td>36865322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-02 16:00:00</th>\n",
       "      <td>40.66</td>\n",
       "      <td>40.74</td>\n",
       "      <td>40.12</td>\n",
       "      <td>40.29</td>\n",
       "      <td>37487476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-06 16:00:00</th>\n",
       "      <td>40.34</td>\n",
       "      <td>41.78</td>\n",
       "      <td>40.18</td>\n",
       "      <td>41.55</td>\n",
       "      <td>39223692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-07 16:00:00</th>\n",
       "      <td>41.61</td>\n",
       "      <td>41.91</td>\n",
       "      <td>41.31</td>\n",
       "      <td>41.53</td>\n",
       "      <td>28809375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-08 16:00:00</th>\n",
       "      <td>41.48</td>\n",
       "      <td>41.69</td>\n",
       "      <td>41.04</td>\n",
       "      <td>41.42</td>\n",
       "      <td>24753438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Open   High    Low  Close    Volume\n",
       "Date                                                     \n",
       "2015-04-01 16:00:00  40.60  40.76  40.31  40.72  36865322\n",
       "2015-04-02 16:00:00  40.66  40.74  40.12  40.29  37487476\n",
       "2015-04-06 16:00:00  40.34  41.78  40.18  41.55  39223692\n",
       "2015-04-07 16:00:00  41.61  41.91  41.31  41.53  28809375\n",
       "2015-04-08 16:00:00  41.48  41.69  41.04  41.42  24753438"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the date column to proper date and time format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "#Now set the Date column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensure all values are numeric or replace it with NaN\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Handle missing values by filling them with the mean of the column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "#display the columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final tensor shape: torch.Size([755, 10, 5])\n",
      "Sample from Final Temporal Tensor:\n",
      " tensor([[[   2,    1,    4,    2, 2015],\n",
      "         [   3,    2,    4,    2, 2015],\n",
      "         [   0,    6,    4,    2, 2015],\n",
      "         [   1,    7,    4,    2, 2015],\n",
      "         [   2,    8,    4,    2, 2015],\n",
      "         [   3,    9,    4,    2, 2015],\n",
      "         [   4,   10,    4,    2, 2015],\n",
      "         [   0,   13,    4,    2, 2015],\n",
      "         [   1,   14,    4,    2, 2015],\n",
      "         [   2,   15,    4,    2, 2015]],\n",
      "\n",
      "        [[   3,   16,    4,    2, 2015],\n",
      "         [   4,   17,    4,    2, 2015],\n",
      "         [   0,   20,    4,    2, 2015],\n",
      "         [   1,   21,    4,    2, 2015],\n",
      "         [   2,   22,    4,    2, 2015],\n",
      "         [   3,   23,    4,    2, 2015],\n",
      "         [   4,   24,    4,    2, 2015],\n",
      "         [   0,   27,    4,    2, 2015],\n",
      "         [   1,   28,    4,    2, 2015],\n",
      "         [   2,   29,    4,    2, 2015]],\n",
      "\n",
      "        [[   3,   30,    4,    2, 2015],\n",
      "         [   4,    1,    5,    2, 2015],\n",
      "         [   0,    4,    5,    2, 2015],\n",
      "         [   1,    5,    5,    2, 2015],\n",
      "         [   2,    6,    5,    2, 2015],\n",
      "         [   3,    7,    5,    2, 2015],\n",
      "         [   4,    8,    5,    2, 2015],\n",
      "         [   0,   11,    5,    2, 2015],\n",
      "         [   1,   12,    5,    2, 2015],\n",
      "         [   2,   13,    5,    2, 2015]],\n",
      "\n",
      "        [[   3,   14,    5,    2, 2015],\n",
      "         [   4,   15,    5,    2, 2015],\n",
      "         [   0,   18,    5,    2, 2015],\n",
      "         [   1,   19,    5,    2, 2015],\n",
      "         [   2,   20,    5,    2, 2015],\n",
      "         [   3,   21,    5,    2, 2015],\n",
      "         [   4,   22,    5,    2, 2015],\n",
      "         [   1,   26,    5,    2, 2015],\n",
      "         [   2,   27,    5,    2, 2015],\n",
      "         [   3,   28,    5,    2, 2015]],\n",
      "\n",
      "        [[   4,   29,    5,    2, 2015],\n",
      "         [   0,    1,    6,    2, 2015],\n",
      "         [   1,    2,    6,    2, 2015],\n",
      "         [   2,    3,    6,    2, 2015],\n",
      "         [   3,    4,    6,    2, 2015],\n",
      "         [   4,    5,    6,    2, 2015],\n",
      "         [   0,    8,    6,    2, 2015],\n",
      "         [   1,    9,    6,    2, 2015],\n",
      "         [   2,   10,    6,    2, 2015],\n",
      "         [   3,   11,    6,    2, 2015]]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Extract temporal attributes\n",
    "day_of_week = torch.tensor(df.index.dayofweek, dtype=torch.int32)  # 0 = Monday, 6 = Sunday\n",
    "day_of_month = torch.tensor(df.index.day, dtype=torch.int32)       # 1 to 31\n",
    "year = torch.tensor(df.index.year, dtype=torch.int32)              # Year\n",
    "month = torch.tensor(df.index.month, dtype=torch.int32)            # Month (1 to 12)\n",
    "quarter = torch.tensor(df.index.quarter, dtype=torch.int32)        # Quarter (1 to 4)\n",
    "\n",
    "# Set parameters\n",
    "num_labels = 5  # Number of labels (repetition factor)\n",
    "patch_length = 10  # Length of each patch\n",
    "\n",
    "# Step 3: Repeat each temporal attribute tensor\n",
    "day_of_week_repeated = day_of_week.repeat(num_labels)\n",
    "day_of_month_repeated = day_of_month.repeat(num_labels)\n",
    "year_repeated = year.repeat(num_labels)\n",
    "month_repeated = month.repeat(num_labels)\n",
    "quarter_repeated = quarter.repeat(num_labels)\n",
    "\n",
    "# Step 4: Define the patching function\n",
    "def patching(x, patch_length):\n",
    "    num_elements = x.shape[0]\n",
    "    num_patches = num_elements // patch_length\n",
    "    x = x[:num_patches * patch_length]  # Ensure the tensor length is a multiple of patch_length\n",
    "    x = x.view(num_patches, patch_length)\n",
    "    return x\n",
    "\n",
    "# Step 5: Apply patching to each repeated tensor\n",
    "day_of_week_patched = patching(day_of_week_repeated, patch_length)\n",
    "day_of_month_patched = patching(day_of_month_repeated, patch_length)\n",
    "year_patched = patching(year_repeated, patch_length)\n",
    "month_patched = patching(month_repeated, patch_length)\n",
    "quarter_patched = patching(quarter_repeated, patch_length)\n",
    "\n",
    "# Step 6: Stack the patched tensors to create the final 3D tensor\n",
    "final_tensor = torch.stack(\n",
    "    (day_of_week_patched, day_of_month_patched, month_patched, quarter_patched, year_patched), \n",
    "    dim=-1\n",
    ")\n",
    "\n",
    "# Verify final shape\n",
    "num_patches, patch_len, num_temporal_attributes = final_tensor.shape\n",
    "print(f\"Final tensor shape: {final_tensor.shape}\")  # Expected: (num_labels * num_data_points / patch_length, patch_length, num_temporal_attributes)\n",
    "\n",
    "# Display first few entries to verify\n",
    "print(\"Sample from Final Temporal Tensor:\\n\", final_tensor[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1511 entries, 2015-04-01 16:00:00 to 2021-03-31 16:00:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Open    1511 non-null   float64\n",
      " 1   High    1511 non-null   float64\n",
      " 2   Low     1511 non-null   float64\n",
      " 3   Close   1511 non-null   float64\n",
      " 4   Volume  1511 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 70.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "\n",
    "#open : the price at which the stock has opened\n",
    "#high : the highest price of a stock\n",
    "#volume: total no of share brought in that stock\n",
    "\n",
    "# all of these are for that particular day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1511, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1781, -1.1794, -1.1788, -1.1764,  0.4678],\n",
      "        [-1.1770, -1.1798, -1.1822, -1.1839,  0.5114],\n",
      "        [-1.1826, -1.1616, -1.1811, -1.1617,  0.6332],\n",
      "        ...,\n",
      "        [ 2.2791,  2.2370,  2.2435,  2.2542, -0.3488],\n",
      "        [ 2.2251,  2.1856,  2.2296,  2.1944, -0.3793],\n",
      "        [ 2.2142,  2.2771,  2.2526,  2.2635,  0.9419]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InstanceNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        std = x.std(dim=0, keepdim=True)\n",
    "        return (x - mean) / std\n",
    "\n",
    "# Convert data to PyTorch tensor\n",
    "time_series = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "# Apply instance normalization\n",
    "instance_norm = InstanceNormalization()\n",
    "normalized_series = instance_norm(time_series)\n",
    "\n",
    "print(normalized_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1511, 5])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Applying channel independence to convert multiple univariate time series data into univariate time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel-Independent Tensor: tensor([[-1.1781],\n",
      "        [-1.1770],\n",
      "        [-1.1826],\n",
      "        ...,\n",
      "        [-0.3488],\n",
      "        [-0.3793],\n",
      "        [ 0.9419]])\n",
      "Shape of channel-independent tensor: torch.Size([7555, 1])\n"
     ]
    }
   ],
   "source": [
    "# Channel-independence\n",
    "def channel_independence(x):\n",
    "    batch_size, num_features = x.shape            #number of elements in total, number of columns\n",
    "    x=x.T\n",
    "    x = x.reshape(batch_size * num_features, 1)   # Combine time steps and features into a single dimension\n",
    "    return x\n",
    "\n",
    "#easier for trend analysis and season analysis in the time series.\n",
    "ci_series = channel_independence(normalized_series)\n",
    "\n",
    "print(\"Channel-Independent Tensor:\", ci_series)\n",
    "print(\"Shape of channel-independent tensor:\", ci_series.shape)  # Should be (1511*5, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Patching to reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched Tensor: tensor([[-1.1781, -1.1770, -1.1826,  ..., -1.1640, -1.1569, -1.1576],\n",
      "        [-1.1543, -1.1592, -1.1581,  ..., -1.0611, -1.0514, -1.0348],\n",
      "        [-1.0352, -1.0373, -1.0410,  ..., -1.0555, -1.0678, -1.0442],\n",
      "        ...,\n",
      "        [-0.5621, -1.0137, -0.9568,  ...,  0.4384,  0.0021, -0.2707],\n",
      "        [ 0.6556,  0.5347, -0.3414,  ...,  0.3556,  0.2022, -0.0317],\n",
      "        [-0.0204, -0.5294, -0.2916,  ..., -0.0050,  0.1010, -0.3212]])\n",
      "Shape of patched tensor: torch.Size([755, 10])\n"
     ]
    }
   ],
   "source": [
    "def patching(x, patch_length):\n",
    "    num_elements = x.shape[0]\n",
    "    num_patches = num_elements // patch_length\n",
    "    x = x[:num_patches * patch_length]  # Ensure the tensor length is a multiple of patch_length\n",
    "    x = x.view(num_patches, patch_length)\n",
    "    return x\n",
    "\n",
    "patch_length = 10  # Example patch length\n",
    "patched_series = patching(ci_series, patch_length)\n",
    "\n",
    "print(\"Patched Tensor:\", patched_series)\n",
    "print(\"Shape of patched tensor:\", patched_series.shape)  # Should be (num_patches, patch_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Three Encodings for Patched Time-Series Data\n",
    "\n",
    "4.1 First part of it will be to employ the new CNN encoding layer for the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token Embeddings Shape: torch.Size([755, 768])\n",
      "Positional Embeddings Shape: tensor([[[ 1.8852, -0.8223,  0.2237,  ..., -1.0703, -0.7582, -0.8828],\n",
      "         [ 0.4551,  0.8455, -2.1381,  ..., -0.5390,  0.6650, -0.2199],\n",
      "         [-1.0357,  1.3273,  0.7391,  ..., -0.2569,  0.2021, -0.1995],\n",
      "         ...,\n",
      "         [ 1.9818, -0.0663, -0.6259,  ..., -0.2487, -1.4778, -1.1108],\n",
      "         [ 0.0617, -0.5910,  0.5233,  ..., -0.2422,  1.8475, -0.0027],\n",
      "         [ 0.7473,  0.4912, -0.4797,  ..., -1.6512, -1.3292,  0.1985]]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n",
      "Positional Embeddings Shape: torch.Size([1, 755, 768])\n",
      "Final Embeddings Shape: torch.Size([1, 755, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a class for the convolution token layer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvToken(nn.Module):\n",
    "    def __init__(self, patch_length, embedding_dim):\n",
    "        super(ConvToken, self).__init__()\n",
    "        # Convolutional layer with kernel size equal to patch length, producing a single embedding per patch\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=embedding_dim, kernel_size=patch_length, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add a channel dimension for Conv1d and apply convolution\n",
    "        x = x.unsqueeze(1)  # Shape: (num_patches, 1, patch_length)\n",
    "        x = self.conv(x)  # Output Shape: (num_patches, embedding_dim, 1)\n",
    "        return x.squeeze(-1)  # Final Shape: (num_patches, embedding_dim)\n",
    "\n",
    "\n",
    "# Define positional encoding class\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create an embedding matrix to learn positional embeddings\n",
    "        # Each position (from 0 to num_patches-1) gets a corresponding embedding of size embedding_dim\n",
    "        self.embedding = nn.Embedding(num_patches, embedding_dim)\n",
    "\n",
    "    # Forward pass method for positional encoding\n",
    "    def forward(self, x):\n",
    "        # Generate positional indices (0, 1, 2, ..., num_patches-1)\n",
    "        # These indices represent the position of each patch\n",
    "        positions = torch.arange(x.size(0), device=x.device).unsqueeze(0)\n",
    "\n",
    "        # Use the positional indices to get the corresponding positional embeddings\n",
    "        # Now we have an embedding for each position in the sequence\n",
    "        return self.embedding(positions)\n",
    "    \n",
    "\n",
    "\n",
    "# Example parameters\n",
    "patch_length = 10  # The length of each patch (i.e., each patch contains data for 2 days, since each day has 5 features)\n",
    "embedding_dim = 768  # The dimension of the embedding space, which is the output size for each patch after the convolution\n",
    "num_patches = patched_series.shape[0]  # The number of patches in the sequence (i.e., how many groups of 2 days we have)\n",
    "\n",
    "# Initialize the ConvToken and PositionalEncoding layers with the specified parameters\n",
    "conv_token = ConvToken(patch_length, embedding_dim)\n",
    "pos_encoding = PositionalEncoding(num_patches, embedding_dim)\n",
    "\n",
    "# Apply the ConvToken layer to the patched_series\n",
    "# This step will learn a local representation for each patch using the convolution layer.\n",
    "# It transforms each patch into an embedding vector of size (batch_size, embedding_dim, num_patches)\n",
    "token_embeddings = conv_token(patched_series)\n",
    "\n",
    "# Apply the PositionalEncoding layer\n",
    "# This will give each patch a positional embedding so the model knows where in the sequence it belongs\n",
    "# The output is of size (num_patches, embedding_dim)\n",
    "pos_embeddings = pos_encoding(token_embeddings)\n",
    "\n",
    "# Combine the token embeddings (representation of each patch) with the positional embeddings\n",
    "# We add them element-wise to give the network both the learned features of each patch and their position\n",
    "final_embeddings = token_embeddings.unsqueeze(0) + pos_embeddings\n",
    "\n",
    "# Print the shapes of the embeddings for clarity\n",
    "print(\"Token Embeddings Shape:\", token_embeddings.shape)  # Shape will be (batch_size, embedding_dim, num_patches)\n",
    "print(\"Positional Embeddings Shape:\", pos_embeddings)  # Shape will be (num_patches, embedding_dim)\n",
    "print(\"Positional Embeddings Shape:\", pos_embeddings.shape)  # Shape will be (num_patches, embedding_dim)\n",
    "\n",
    "print(\"Final Embeddings Shape:\", final_embeddings.shape)  # The final shape will be (batch_size, embedding_dim, num_patches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Creating the temporal encoding layer to address the challenge LLMs face in processing multi-scale temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporal Embedding Tensor: tensor([[ 3.2959,  3.4124,  0.8670,  ...,  1.9218, -1.5223,  2.7538],\n",
      "        [-0.0595,  1.6356,  2.4412,  ..., -1.2818,  2.1425,  2.2616],\n",
      "        [ 0.9371,  2.3437, -0.8249,  ..., -1.1932,  1.3747,  1.4913],\n",
      "        ...,\n",
      "        [ 1.4053,  0.5551,  1.7479,  ..., -0.1677,  2.1115,  4.4237],\n",
      "        [-1.3036, -0.6486,  4.9377,  ...,  1.4821,  3.7897,  4.6646],\n",
      "        [-1.6713, -0.5499,  1.9043,  ...,  0.0608, -1.1839,  0.4542]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Shape of Temporal Embedding Tensor: torch.Size([755, 768])\n",
      "Final Embeddings Shape: tensor([[[ 6.3368,  2.6870,  0.1753,  ..., -0.8051, -1.7678,  0.7859],\n",
      "         [ 1.4813,  2.6287, -0.5777,  ..., -3.4035,  3.2931,  1.0017],\n",
      "         [ 0.9265,  3.7791, -0.8962,  ..., -2.9691,  2.0003,  0.2812],\n",
      "         ...,\n",
      "         [ 3.2997,  1.3091,  0.4971,  ..., -1.2503,  0.8664,  2.8658],\n",
      "         [-1.5604, -1.1791,  6.0538,  ...,  1.3334,  5.1060,  4.4530],\n",
      "         [-1.3013,  0.2671,  1.7665,  ..., -1.9491, -2.8182,  0.1010]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "Final Embeddings Shape: torch.Size([1, 755, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, embedding_dim, max_values):\n",
    "            \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_patches (int): Number of patches in the dataset.\n",
    "            embedding_dim (int): Dimension of each embedding vector.\n",
    "            max_values (dict): Dictionary specifying max values for each temporal attribute.\n",
    "                               For example: {'sec': 59, 'min': 59, 'hour': 23, 'day': 31, 'month': 12, 'year': 2100}\n",
    "        \"\"\"\n",
    "        super(TemporalEncoding, self).__init__()\n",
    "        \n",
    "        # Create trainable embeddings for each temporal attribute\n",
    "        self.embeddings = nn.ModuleDict({\n",
    "            attr: nn.Embedding(max_val + 1, embedding_dim)\n",
    "            for attr, max_val in max_values.items()\n",
    "        })\n",
    "\n",
    "        # Pooling method: Select the first timestamp in each patch\n",
    "        self.pooling = nn.AdaptiveMaxPool1d(1)  # Can replace with other pooling methods if needed\n",
    "\n",
    "    def forward(self, temporal_attributes_tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            temporal_attributes_tensor (torch.Tensor): Tensor of shape (num_patches, num_timestamps, num_attributes)\n",
    "                                                      containing temporal attributes.\n",
    "                                                      Example: [day_of_week, day_of_month, year, month, quarter]\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Temporal embedding tensor of shape (num_patches, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Extract embeddings for each temporal attribute and sum them (Level 1 Aggregation)\n",
    "        temporal_embedding = 0\n",
    "        for i, (attr, embedding_layer) in enumerate(self.embeddings.items()):\n",
    "            attr_values = temporal_attributes_tensor[:, :, i]  # Shape: (num_patches, num_timestamps)\n",
    "            temporal_embedding += embedding_layer(attr_values)  # Shape: (num_patches, num_timestamps, embedding_dim)\n",
    "        \n",
    "        # Level 2 Aggregation: Pool across timestamps within each patch\n",
    "        # Assuming we are using \"select first\" as the pooling method\n",
    "        temporal_embedding = temporal_embedding[:, 0, :]  # Select the first timestamp in each patch\n",
    "        # Shape after pooling: (num_patches, embedding_dim)\n",
    "\n",
    "        return temporal_embedding\n",
    "\n",
    "# Example usage\n",
    "# Suppose temporal_attributes_tensor is a tensor with shape (num_patches, num_timestamps, num_attributes)\n",
    "# containing [day_of_week, day_of_month, year, month, quarter]\n",
    "\n",
    "# Define max values for each temporal attribute\n",
    "max_values = {\n",
    "    'day_of_week': 6,   # 0 = Monday, 6 = Sunday\n",
    "    'day_of_month': 31, # 1 to 31\n",
    "    'month': 12,        # 1 to 12\n",
    "    'quarter': 4,       # 1 to 4\n",
    "    'year': 2025 # Assuming years in range 2000-2100\n",
    "}\n",
    "\n",
    "embedding_dim = 768  # Example embedding dimension\n",
    "num_patches = patched_series.size(0)  # Example number of patches\n",
    "num_timestamps = patch_length  # Example number of timestamps per patch\n",
    "num_attributes = len(max_values)\n",
    "\n",
    "# Example temporal attributes tensor\n",
    "temporal_attributes_tensor = final_tensor #shape (num_patches, num_timestamps, num_attributes)\n",
    "\n",
    "# # Initialize and apply the TemporalEncoding layer\n",
    "temporal_encoder = TemporalEncoding(num_patches, embedding_dim, max_values)\n",
    "temporal_embedding = temporal_encoder(temporal_attributes_tensor)\n",
    "\n",
    "print(\"Temporal Embedding Tensor:\", temporal_embedding)\n",
    "\n",
    "print(\"Shape of Temporal Embedding Tensor:\", temporal_embedding.shape)  # Expected shape: (num_patches,Â embedding_dim)\n",
    "\n",
    "final_embeddings = final_embeddings + temporal_embedding.unsqueeze(0)\n",
    "print(\"Final Embeddings Shape:\", final_embeddings)  \n",
    "print(\"Final Embeddings Shape:\", final_embeddings.shape)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Now preserving the parameters for the pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import copy\n",
    "# from huggingface_hub import hf_hub_download\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # Set up the access token\n",
    "# HUGGING_FACE_API_KEY = \"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\"\n",
    "\n",
    "# # Define model ID and file names\n",
    "# model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "# filenames = [\n",
    "#     \"LICENSE\",\n",
    "#     \"README.md\",\n",
    "#     \"USE_POLICY.md\",\n",
    "#     \"config.json\",\n",
    "#     \"generation_config.json\",\n",
    "#     \"model-00001-of-00004.safetensors\",\n",
    "#     \"model-00002-of-00004.safetensors\",\n",
    "#     \"model-00003-of-00004.safetensors\",\n",
    "#     \"model-00004-of-00004.safetensors\",\n",
    "#     \"model.safetensors.index.json\",\n",
    "#     \"special_tokens_map.json\",\n",
    "#     \"tokenizer.json\",\n",
    "#     \"tokenizer_config.json\"\n",
    "# ]\n",
    "\n",
    "# # Download necessary model files (this is only needed if not automatically handled)\n",
    "# for filename in filenames:\n",
    "#     download_model_path = hf_hub_download(\n",
    "#         repo_id=model_id,\n",
    "#         filename=filename,\n",
    "#         token=HUGGING_FACE_API_KEY\n",
    "#     )\n",
    "#     print(\"Downloaded:\", download_model_path)\n",
    "\n",
    "# # Load the tokenizer and model\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=HUGGING_FACE_API_KEY)\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=HUGGING_FACE_API_KEY)\n",
    "\n",
    "# # Set padding token if not already set\n",
    "# tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "# pretrained_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# # Custom PretrainedLLM model\n",
    "# # class PretrainedLLM(nn.Module):\n",
    "# #     def __init__(self, pretrained_model, num_layers, embedding_dim, prediction_dim):\n",
    "# #         super(PretrainedLLM, self).__init__()\n",
    "# #         # Freeze most parameters of the pre-trained model\n",
    "# #         for param in pretrained_model.parameters():\n",
    "# #             param.requires_grad = False\n",
    "\n",
    "# #         # Extract specific layers from the pre-trained model\n",
    "# #         # Extract specific layers from the pre-trained model for LLaMA\n",
    "# #         self.transformer_blocks = nn.ModuleList([copy.deepcopy(pretrained_model.model.layers[i]) for i in range(num_layers)])\n",
    "\n",
    "\n",
    "# #         # Linear output layer\n",
    "# #         self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "# #     def forward(self, embeddings):\n",
    "# #         # Pass embeddings through selected Transformer blocks\n",
    "# #         for transformer_block in self.transformer_blocks:\n",
    "# #             embeddings = transformer_block(embeddings)[0]  # Only take hidden states\n",
    "\n",
    "# #         # Linear output layer\n",
    "# #         output = self.linear(embeddings)\n",
    "# #         return output\n",
    "\n",
    "# class PretrainedLLM(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_layers, embedding_dim, prediction_dim):\n",
    "#         super(PretrainedLLM, self).__init__()\n",
    "        \n",
    "#         # Freeze pretrained model parameters\n",
    "#         for param in pretrained_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "            \n",
    "#         # Get LLaMA's hidden dimension\n",
    "#         self.llama_hidden_dim = pretrained_model.config.hidden_size  # Usually 4096 for LLaMA\n",
    "        \n",
    "#         # Project input embeddings to LLaMA's hidden dimension\n",
    "#         self.input_projection = nn.Linear(embedding_dim, self.llama_hidden_dim)\n",
    "        \n",
    "#         # Extract specific layers from the pre-trained model\n",
    "#         self.transformer_blocks = nn.ModuleList([\n",
    "#             copy.deepcopy(pretrained_model.model.layers[i]) \n",
    "#             for i in range(num_layers)\n",
    "#         ])\n",
    "        \n",
    "#         # Project from LLaMA's hidden dimension to prediction dimension\n",
    "#         self.output_projection = nn.Linear(self.llama_hidden_dim, prediction_dim)\n",
    "        \n",
    "#         # Layer normalization for input and output\n",
    "#         self.input_norm = nn.LayerNorm(self.llama_hidden_dim)\n",
    "#         self.output_norm = nn.LayerNorm(prediction_dim)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#         # Project embeddings to LLaMA's hidden dimension\n",
    "#         x = self.input_projection(embeddings)\n",
    "#         x = self.input_norm(x)\n",
    "        \n",
    "#         # Prepare attention mask (assuming no padding)\n",
    "#         batch_size = x.size(0)\n",
    "#         attention_mask = torch.ones(batch_size, x.size(1), device=x.device)\n",
    "        \n",
    "#         # Pass through transformer blocks\n",
    "#         for block in self.transformer_blocks:\n",
    "#             # Create the expected input structure for LLaMA layers\n",
    "#             transformer_outputs = block(\n",
    "#                 hidden_states=x,\n",
    "#                 attention_mask=attention_mask,\n",
    "#                 position_ids=None,\n",
    "#                 past_key_value=None,\n",
    "#                 output_attentions=False,\n",
    "#                 use_cache=False,\n",
    "#             )\n",
    "#             x = transformer_outputs[0]\n",
    "        \n",
    "#         # Project to prediction dimension\n",
    "#         x = self.output_projection(x)\n",
    "#         output = self.output_norm(x)\n",
    "        \n",
    "#         return output\n",
    "\n",
    "# # Example parameters\n",
    "# num_layers = 12           # Number of Transformer blocks to use\n",
    "# embedding_dim = 128       # Dimension of the embeddings\n",
    "# num_patches = 755         # Number of patches\n",
    "# prediction_dim = 128      # Dimension of prediction (output)\n",
    "\n",
    "# # Initialize PretrainedLLM\n",
    "# pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, prediction_dim)\n",
    "\n",
    "# # Example input embeddings\n",
    "# embeddings = torch.randn(32,num_patches, embedding_dim)\n",
    "\n",
    "# # Forward pass through the custom model\n",
    "# output = pretrained_llm(embeddings)\n",
    " \n",
    "# # Example ground truth shifted patched data\n",
    "# p_shifted = torch.randn(32,num_patches, prediction_dim)\n",
    "\n",
    "# # Calculate Mean Squared Error (MSE) loss\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss = loss_fn(output, p_shifted)\n",
    "\n",
    "# print(\"MSE Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# class PretrainedLLM(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "#         super(PretrainedLLM, self).__init__()\n",
    "#         # Freeze most parameters of the pre-trained model\n",
    "#         for param in pretrained_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         # Extract the necessary layers from the pre-trained model\n",
    "#         self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "#         # Linear output layer\n",
    "#         self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#         # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "#         for transformer_block in self.transformer_blocks:\n",
    "#             embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "#         # Apply linear output layer\n",
    "#         output = self.linear(embeddings)\n",
    "\n",
    "#         return output\n",
    "\n",
    "# # Load the GPT-2 model and tokenizer\n",
    "# model_id = \"openai-community/gpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# # Example parameters\n",
    "# num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "# embedding_dim = 768  # Dimension of the embeddings\n",
    "# num_patches = 512  # Number of patches\n",
    "# prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# # Initialize PretrainedLLM\n",
    "# pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# # Example input: embeddings after temporal encoding\n",
    "# embeddings = torch.randn(1, num_patches, embedding_dim)  # Add a batch dimension\n",
    "\n",
    "# # Forward pass\n",
    "# output = pretrained_llm(embeddings)\n",
    "\n",
    "# # Example ground truth shifted patched data\n",
    "# p_shifted = torch.randn(1, num_patches, prediction_dim)  # Add a batch dimension\n",
    "\n",
    "# # Calculate Mean Squared Error (MSE) loss\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss = loss_fn(output, p_shifted)\n",
    "\n",
    "# print(\"MSE Loss:\", loss.item())'''\n",
    "\n",
    "# # Code with MSE loss of 70\n",
    "\n",
    "# '''import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# class PretrainedLLM(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "#         super(PretrainedLLM, self).__init__()\n",
    "#         # Extract the necessary layers from the pre-trained model\n",
    "#         self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "#         # Additional layers for fine-tuning\n",
    "#         self.dropout = nn.Dropout(0.1)  # Add dropout for regularization\n",
    "#         self.linear1 = nn.Linear(embedding_dim, prediction_dim)  # Additional linear layer\n",
    "#         self.activation = nn.ReLU()  # Additional activation function\n",
    "#         self.linear2 = nn.Linear(prediction_dim, prediction_dim)  # Additional linear layer\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#         # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "#         for transformer_block in self.transformer_blocks:\n",
    "#             embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "#         # Apply additional layers\n",
    "#         embeddings = self.dropout(embeddings)\n",
    "#         embeddings = self.linear1(embeddings)\n",
    "#         embeddings = self.activation(embeddings)\n",
    "#         embeddings = self.linear2(embeddings)\n",
    "\n",
    "#         return embeddings\n",
    "\n",
    "# # Load the GPT-2 model and tokenizer\n",
    "# model_id = \"openai-community/gpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# # Example parameters\n",
    "# num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "# embedding_dim = 768  # Dimension of the embeddings\n",
    "# num_patches = 512  # Number of patches\n",
    "# prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# # Initialize PretrainedLLM\n",
    "# pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# # Example input: embeddings after temporal encoding\n",
    "# embeddings = torch.randn(1, num_patches, embedding_dim)  # Add a batch dimension\n",
    "\n",
    "# # Forward pass\n",
    "# output = pretrained_llm(embeddings)\n",
    "\n",
    "# # Example ground truth shifted patched data\n",
    "# p_shifted = torch.randn(1, num_patches, prediction_dim)  # Add a batch dimension\n",
    "\n",
    "# # Calculate Mean Squared Error (MSE) loss\n",
    "# loss_fn = nn.MSELoss()\n",
    "# loss = loss_fn(output, p_shifted)\n",
    "\n",
    "# print(\"MSE Loss:\", loss.item())'''\n",
    "\n",
    "# #Most updated code :---\n",
    "# '''import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, get_scheduler\n",
    "\n",
    "# class PretrainedLLM(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "#         super(PretrainedLLM, self).__init__()\n",
    "#         # Freeze most parameters of the pre-trained model\n",
    "#         for param in pretrained_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         # Extract the necessary layers from the pre-trained model\n",
    "#         self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "#         # Linear output layer\n",
    "#         self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#         # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "#         for transformer_block in self.transformer_blocks:\n",
    "#             embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "#         # Apply linear output layer\n",
    "#         output = self.linear(embeddings)\n",
    "\n",
    "#         return output\n",
    "\n",
    "# # Load the GPT-2 model and tokenizer\n",
    "# model_id = \"openai-community/gpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# # Example parameters\n",
    "# num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "# embedding_dim = 768  # Dimension of the embeddings\n",
    "# num_patches = 512  # Number of patches\n",
    "# prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# # Initialize PretrainedLLM\n",
    "# pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)'''\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModelForCausalLM\n",
    "# from torch.optim import AdamW\n",
    "# from transformers.optimization import get_scheduler\n",
    "\n",
    "# # PretrainedLLM class that builds on top of a pretrained language model (like GPT-2)\n",
    "# class PretrainedLLM(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim, num_additional_layers=1, hidden_dim=512, dropout_rate=0.1, activation='relu'):\n",
    "#         super(PretrainedLLM, self).__init__()\n",
    "\n",
    "#         # Use a pre-trained model (like GPT-2) and extract its transformer blocks\n",
    "#         self.pretrained_model = pretrained_model\n",
    "#         self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "#         # Additional fully connected layers after the transformer blocks\n",
    "#         # These are added to enhance the output of the pre-trained model\n",
    "#         self.additional_layers = nn.ModuleList()\n",
    "#         for _ in range(num_additional_layers):\n",
    "#             layer = nn.Sequential(\n",
    "#                 nn.Linear(embedding_dim, hidden_dim),  # Linear layer to project embeddings to hidden_dim\n",
    "#                 nn.Dropout(dropout_rate),  # Dropout for regularization (prevent overfitting)\n",
    "#                 nn.ReLU() if activation == 'relu' else nn.LeakyReLU(),  # Non-linear activation function\n",
    "#                 nn.Linear(hidden_dim, embedding_dim)  # Linear layer to project back to embedding_dim\n",
    "#             )\n",
    "#             self.additional_layers.append(layer)\n",
    "\n",
    "#         # Final linear layer to produce the prediction output\n",
    "#         self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#         # Pass the input embeddings through the transformer blocks\n",
    "#         for transformer_block in self.transformer_blocks:\n",
    "#             embeddings = transformer_block(embeddings)[0]  # Transformer block output, keeping only the embeddings\n",
    "\n",
    "#         # Apply additional layers (fully connected layers) to the embeddings\n",
    "#         for layer in self.additional_layers:\n",
    "#             embeddings = layer(embeddings)\n",
    "\n",
    "#         # Final output through the linear layer to predict the desired output\n",
    "#         output = self.linear(embeddings)\n",
    "#         return output\n",
    "\n",
    "# # Example usage\n",
    "# # Load a pre-trained GPT-2 model\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# # Set the number of transformer layers (12 in GPT-2), embedding dimension (768), and number of patches (512)\n",
    "# num_layers = 12\n",
    "# embedding_dim = 768\n",
    "# num_patches = 512\n",
    "# prediction_dim = 768  # The dimensionality of the output prediction\n",
    "\n",
    "# # Additional settings for the fully connected layers added to the model\n",
    "# num_additional_layers = 2  # Adding 2 extra layers for better predictions\n",
    "# hidden_dim = 512  # The size of the hidden layers\n",
    "# dropout_rate = 0.1  # Dropout rate for regularization\n",
    "# activation = 'relu'  # Activation function to use (ReLU)\n",
    "\n",
    "# # Initialize the PretrainedLLM model\n",
    "# pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim,\n",
    "#                                num_additional_layers=num_additional_layers, hidden_dim=hidden_dim,\n",
    "#                                dropout_rate=dropout_rate, activation=activation)\n",
    "\n",
    "# # Example input: Random tensor representing embeddings after temporal encoding (shape: [batch_size, num_patches, embedding_dim])\n",
    "# embeddings = torch.randn(1, num_patches, embedding_dim)  # Adding a batch dimension (size 1)\n",
    "\n",
    "# # Example ground truth: Shifted patched data for prediction task (shape: [batch_size, num_patches, prediction_dim])\n",
    "# p_shifted = torch.randn(1, num_patches, prediction_dim)  # Similar batch size and patch dimension as input\n",
    "\n",
    "# # Training parameters\n",
    "# batch_size = 16  # Size of the mini-batches for training\n",
    "# num_epochs = 10  # Number of training iterations (epochs)\n",
    "# learning_rate = 5e-5  # Learning rate for the optimizer\n",
    "\n",
    "# # Define optimizer (AdamW is often used for transformer models) and a learning rate scheduler\n",
    "# optimizer = AdamW(pretrained_llm.parameters(), lr=learning_rate)\n",
    "# scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=num_epochs // 10, num_training_steps=num_epochs)\n",
    "\n",
    "# # Loss function: Mean Squared Error (MSE) for regression-like tasks\n",
    "# loss_fn = nn.MSELoss()\n",
    "\n",
    "# # Training loop to train the model over several epochs\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass: Get model predictions from input embeddings\n",
    "#     output = pretrained_llm(embeddings)\n",
    "\n",
    "#     # Calculate the loss by comparing the model output with the true data (p_shifted)\n",
    "#     loss = loss_fn(output, p_shifted)\n",
    "\n",
    "#     # Backpropagation: Zero the gradients, compute gradients, and update weights\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     scheduler.step()\n",
    "\n",
    "#     # Print loss for each epoch to track training progress\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# # Final evaluation of the model after training\n",
    "# output = pretrained_llm(embeddings)\n",
    "# final_loss = loss_fn(output, p_shifted)\n",
    "# print(\"Final MSE Loss:\", final_loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python311\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 10.830574989318848\n",
      "Epoch 2/10, Loss: 10.830574989318848\n",
      "Epoch 3/10, Loss: 10.618239402770996\n",
      "Epoch 4/10, Loss: 10.432013511657715\n",
      "Epoch 5/10, Loss: 10.271047592163086\n",
      "Epoch 6/10, Loss: 10.134566307067871\n",
      "Epoch 7/10, Loss: 10.021904945373535\n",
      "Epoch 8/10, Loss: 9.932487487792969\n",
      "Epoch 9/10, Loss: 9.865854263305664\n",
      "Epoch 10/10, Loss: 9.821646690368652\n",
      "Final MSE Loss: 9.799612998962402\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Model, AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "class FineTunedLLM(nn.Module):\n",
    "    def __init__(self, embedding_dim, patch_dim, lora_rank=4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            embedding_dim (int): Dimension of the input embeddings.\n",
    "            patch_dim (int): Dimension of the output patch, matching the original patch length.\n",
    "            lora_rank (int): Rank for LoRA matrices in PEFT.\n",
    "        \"\"\"\n",
    "        super(FineTunedLLM, self).__init__()\n",
    "        \n",
    "        # Load pre-trained GPT-2 model\n",
    "        self.llm = GPT2Model.from_pretrained(\"gpt2\")\n",
    "        \n",
    "        # Freeze all parameters initially, then selectively unfreeze LayerNorm and LoRA\n",
    "        for name, param in self.llm.named_parameters():\n",
    "            param.requires_grad = False  # Freeze all parameters\n",
    "        \n",
    "        # Enable LayerNorm tuning by making its affine parameters trainable\n",
    "        for name, module in self.llm.named_modules():\n",
    "            if isinstance(module, nn.LayerNorm):\n",
    "                module.weight.requires_grad = True\n",
    "                module.bias.requires_grad = True\n",
    "        \n",
    "        # Apply LoRA to self-attention layers (Query and Key matrices)\n",
    "        self.apply_lora(self.llm, lora_rank)\n",
    "        \n",
    "        # Define linear output layer to map LLM output to target patch dimension\n",
    "        self.output_layer = nn.Linear(embedding_dim, patch_dim)\n",
    "\n",
    "    def apply_lora(self, model, rank):\n",
    "        \"\"\"\n",
    "        Applies Low-Rank Adaptation (LoRA) to the self-attention mechanism's Q and K matrices.\n",
    "        \"\"\"\n",
    "        for name, module in model.named_modules():\n",
    "            if hasattr(module, 'attn'):\n",
    "                # Apply LoRA to Query (Q) and Key (K) projections\n",
    "                if hasattr(module.attn, 'q_proj'):\n",
    "                    module.attn.q_proj_lora = nn.Linear(module.attn.q_proj.in_features, rank, bias=False)\n",
    "                    module.attn.q_proj_lora.weight.requires_grad = True\n",
    "                if hasattr(module.attn, 'k_proj'):\n",
    "                    module.attn.k_proj_lora = nn.Linear(module.attn.k_proj.in_features, rank, bias=False)\n",
    "                    module.attn.k_proj_lora.weight.requires_grad = True\n",
    "\n",
    "    def forward(self, embedding_input):\n",
    "        # Pass input through the pre-trained LLM with PEFT\n",
    "        output = self.llm(inputs_embeds=embedding_input).last_hidden_state\n",
    "        # Transform the output to predict shifted patches\n",
    "        predicted_patch = self.output_layer(output)\n",
    "        return predicted_patch\n",
    "\n",
    "# Model parameters\n",
    "embedding_dim = 768  # Adjusted embedding dimension after processing\n",
    "patch_dim = 10       # Dimension of the target patch\n",
    "lora_rank = 4        # LoRA rank\n",
    "\n",
    "# Initialize the fine-tuned model\n",
    "model = FineTunedLLM(embedding_dim, patch_dim, lora_rank)\n",
    "\n",
    "# Example inputs\n",
    "batch_size = 1       # Example batch size\n",
    "num_patches = 755    # Example sequence length\n",
    "\n",
    "# Assume final_embeddings has been calculated, e.g., from temporal encoding\n",
    "used_embedings = final_embeddings  # Adjusted embeddings\n",
    "\n",
    "# Shift the actual patch data by one position for autoregressive target\n",
    "patched_series = patched_series  # Replace with actual patched data\n",
    "shifted_patched_series = torch.roll(patched_series, shifts=1, dims=0).unsqueeze(0)\n",
    "\n",
    "# Expand to match batch size for the training loop\n",
    "actual_shifted_patch = shifted_patched_series.expand(batch_size, -1, -1)\n",
    "\n",
    "# Training parameters\n",
    "learning_rate = 5e-5\n",
    "num_epochs = 10\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=num_epochs // 10, num_training_steps=num_epochs)\n",
    "\n",
    "# Loss function: Mean Squared Error (MSE)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: Get model predictions from input embeddings\n",
    "    output_prediction = model(used_embedings)\n",
    "    \n",
    "    # Calculate the MSE loss with actual shifted patches\n",
    "    loss = loss_fn(output_prediction, actual_shifted_patch)\n",
    "    \n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward(retain_graph=True)  # Retain graph for multiple backward passes if needed\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Print loss for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Final evaluation after training\n",
    "final_output = model(used_embedings)\n",
    "final_loss = loss_fn(final_output, actual_shifted_patch)\n",
    "print(\"Final MSE Loss:\", final_loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 forcasting finetuning\n",
    "+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# # Define the reversible instance normalization (RevIN) class\n",
    "# class RevIN(nn.Module):\n",
    "#     def __init__(self, num_features, affine=True):\n",
    "#         super(RevIN, self).__init__()\n",
    "#         self.num_features = num_features  # Number of features in the input data\n",
    "#         # InstanceNorm1d applies normalization to the features across a batch\n",
    "#         self.norm = nn.InstanceNorm1d(num_features, affine=affine)\n",
    "#         # Learnable scale and shift parameters to adjust the normalized data\n",
    "#         self.affine_scale = nn.Parameter(torch.ones(1, num_features, 1))\n",
    "#         self.affine_shift = nn.Parameter(torch.zeros(1, num_features, 1))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Transpose the input so that it's in the correct shape for normalization\n",
    "#         x_transposed = x.transpose(1, 2)  # (batch_size, num_features, seq_len)\n",
    "#         # Apply instance normalization to standardize the data\n",
    "#         normed = self.norm(x_transposed)  # (batch_size, num_features, seq_len)\n",
    "#         # Apply scaling and shifting to normalized data\n",
    "#         normed = normed * self.affine_scale + self.affine_shift\n",
    "#         # Transpose the data back to the original shape\n",
    "#         normed = normed.transpose(1, 2)  # (batch_size, seq_len, num_features)\n",
    "\n",
    "#         return normed\n",
    "\n",
    "# # Main Forecasting Model that includes the RevIN and GPT-2 based transformer\n",
    "# class ForecastingModel(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "#         super(ForecastingModel, self).__init__()\n",
    "#         # Reversible instance normalization (RevIN) to normalize input data\n",
    "#         self.revin = RevIN(embedding_dim)\n",
    "\n",
    "#         # Select the transformer blocks (layers) from the pretrained GPT-2 model\n",
    "#         self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "#         # Additional layers for prediction: a dropout, a linear layer, and an activation function (ReLU)\n",
    "#         self.dropout = nn.Dropout(0.1)  # Dropout for regularization (prevents overfitting)\n",
    "#         self.linear1 = nn.Linear(embedding_dim, embedding_dim)  # First linear transformation\n",
    "#         self.activation = nn.ReLU()  # Activation function to introduce non-linearity\n",
    "#         self.linear2 = nn.Linear(embedding_dim, prediction_dim)  # Second linear transformation for prediction\n",
    "\n",
    "#         # Output layer to transform the prediction back to the original embedding dimension\n",
    "#         self.output_linear = nn.Linear(prediction_dim, embedding_dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Apply normalization using RevIN\n",
    "#         x = self.revin(x)\n",
    "\n",
    "#         # Pass the normalized data through each transformer block (pretrained GPT-2 layers)\n",
    "#         for transformer_block in self.transformer_blocks:\n",
    "#             x = transformer_block(x)[0]  # Extract the output from each block\n",
    "\n",
    "#         # Apply the additional prediction layers (dropout -> linear -> activation -> linear)\n",
    "#         x = self.dropout(x)\n",
    "#         x = self.linear1(x)\n",
    "#         x = self.activation(x)\n",
    "#         x = self.linear2(x)\n",
    "\n",
    "#         # Final transformation and denormalize the output using RevIN\n",
    "#         x = self.output_linear(x)\n",
    "#         x = self.revin(x)  # Reverse the normalization to get data back in its original form\n",
    "\n",
    "#         return x\n",
    "\n",
    "# # Load the pretrained GPT-2 model and tokenizer for later use\n",
    "# model_id = \"openai-community/gpt2\"\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "# pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# # Define the model parameters\n",
    "# num_layers = 12  # Use 12 transformer layers from GPT-2\n",
    "# embedding_dim = 768  # The dimension of the input embeddings\n",
    "# num_patches = 512  # The number of time steps (sequence length)\n",
    "# prediction_dim = 768  # Prediction output dimension (same as input dimension)\n",
    "\n",
    "# # Initialize the forecasting model with the GPT-2 transformer\n",
    "# forecasting_model = ForecastingModel(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# # Example input: random data to simulate temporal embeddings (e.g., stock price data)\n",
    "# embeddings = torch.randn(1, num_patches, embedding_dim)  # Random input with batch size 1\n",
    "\n",
    "# # Perform a forward pass through the model to get predictions\n",
    "# output = forecasting_model(embeddings)\n",
    "\n",
    "# # Example of ground truth future data (what we want to predict)\n",
    "# future_data = torch.randn(1, num_patches, embedding_dim)  # Random target data for loss calculation\n",
    "\n",
    "# # Define Mean Squared Error (MSE) as the loss function\n",
    "# loss_fn = nn.MSELoss()\n",
    "# # Calculate the loss between the model output (predictions) and future data\n",
    "# loss = loss_fn(output, future_data)\n",
    "\n",
    "# print(\"MSE Loss:\", loss.item())\n",
    "\n",
    "# # Fine-tuning the model (training the model to minimize the loss)\n",
    "# optimizer = torch.optim.Adam(forecasting_model.parameters(), lr=1e-4)  # Adam optimizer for gradient descent\n",
    "# num_epochs = 10  # Number of times to train on the entire dataset\n",
    "\n",
    "# # Training loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     forecasting_model.train()  # Set the model to training mode\n",
    "#     optimizer.zero_grad()  # Zero out gradients from the previous step\n",
    "#     output = forecasting_model(embeddings)  # Forward pass through the model\n",
    "#     loss = loss_fn(output, future_data)  # Calculate loss\n",
    "#     loss.backward()  # Backpropagate the loss to compute gradients\n",
    "#     optimizer.step()  # Update the model weights using gradients\n",
    "#     print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")  # Output training progress\n",
    "\n",
    "# # Final loss after training\n",
    "# print(\"Final MSE Loss:\", loss.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
