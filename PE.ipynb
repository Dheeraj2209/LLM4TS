{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import hf_hub_download\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")\n",
    "#second commit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGING_FACE_API_KEY = os.environ.get(\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1511 entries, 0 to 1510\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Date    1511 non-null   object \n",
      " 1   Open    1511 non-null   float64\n",
      " 2   High    1511 non-null   float64\n",
      " 3   Low     1511 non-null   float64\n",
      " 4   Close   1511 non-null   float64\n",
      " 5   Volume  1511 non-null   int64  \n",
      "dtypes: float64(4), int64(1), object(1)\n",
      "memory usage: 71.0+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd   # a library for data analysis and manipulation\n",
    "import torch          # qa library for ML\n",
    "import transformers   # a library for NLP techniques from Hugging Face.\n",
    "\n",
    "#what is hugging face?\n",
    "#It is an open source community that has built a library called transformers that has all pre trined models\n",
    "\n",
    "#read the proper CSV file\n",
    "df=pd.read_csv(\"Microsoft_Stock.csv\")\n",
    "\n",
    "\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-04-01 16:00:00</th>\n",
       "      <td>40.60</td>\n",
       "      <td>40.76</td>\n",
       "      <td>40.31</td>\n",
       "      <td>40.72</td>\n",
       "      <td>36865322</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-02 16:00:00</th>\n",
       "      <td>40.66</td>\n",
       "      <td>40.74</td>\n",
       "      <td>40.12</td>\n",
       "      <td>40.29</td>\n",
       "      <td>37487476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-06 16:00:00</th>\n",
       "      <td>40.34</td>\n",
       "      <td>41.78</td>\n",
       "      <td>40.18</td>\n",
       "      <td>41.55</td>\n",
       "      <td>39223692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-07 16:00:00</th>\n",
       "      <td>41.61</td>\n",
       "      <td>41.91</td>\n",
       "      <td>41.31</td>\n",
       "      <td>41.53</td>\n",
       "      <td>28809375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-04-08 16:00:00</th>\n",
       "      <td>41.48</td>\n",
       "      <td>41.69</td>\n",
       "      <td>41.04</td>\n",
       "      <td>41.42</td>\n",
       "      <td>24753438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Open   High    Low  Close    Volume\n",
       "Date                                                     \n",
       "2015-04-01 16:00:00  40.60  40.76  40.31  40.72  36865322\n",
       "2015-04-02 16:00:00  40.66  40.74  40.12  40.29  37487476\n",
       "2015-04-06 16:00:00  40.34  41.78  40.18  41.55  39223692\n",
       "2015-04-07 16:00:00  41.61  41.91  41.31  41.53  28809375\n",
       "2015-04-08 16:00:00  41.48  41.69  41.04  41.42  24753438"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the date column to proper date and time format\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "#Now set the Date column as the index\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Ensure all values are numeric or replace it with NaN\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Handle missing values by filling them with the mean of the column\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "#display the columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched Temporal Tensor Shape: torch.Size([5, 151, 10])\n",
      "Patched Temporal Tensor:\n",
      " tensor([[2, 3, 0,  ..., 0, 1, 2],\n",
      "        [3, 4, 0,  ..., 0, 1, 2],\n",
      "        [3, 4, 0,  ..., 0, 1, 2],\n",
      "        ...,\n",
      "        [2, 3, 4,  ..., 4, 0, 1],\n",
      "        [2, 3, 4,  ..., 4, 0, 1],\n",
      "        [2, 3, 4,  ..., 4, 0, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# Extract temporal attributes (day of the week, day of the month, year, month, quarter)\n",
    "day_of_week = df.index.dayofweek   # 0 = Monday, 6 = Sunday\n",
    "day_of_month = df.index.day        # 1 to 31\n",
    "year = df.index.year               # Year of the date\n",
    "month = df.index.month             # Month of the year (1 to 12)\n",
    "quarter = df.index.quarter         # Quarter of the year (1 to 4)\n",
    "\n",
    "# Create a 2D array with these attributes\n",
    "temporal_attributes_array = list(zip(day_of_week, day_of_month, year, month, quarter))\n",
    "\n",
    "# Convert the 2D array into a PyTorch tensor\n",
    "temporal_attributes_tensor = torch.tensor(temporal_attributes_array, dtype=torch.int32)\n",
    "\n",
    "# Define the patching function\n",
    "def patching(x, patch_length):\n",
    "    num_elements = x.shape[0]\n",
    "    num_patches = num_elements // patch_length\n",
    "    x = x[:num_patches * patch_length]  # Ensure the tensor length is a multiple of patch_length\n",
    "    x = x.view(num_patches, patch_length)\n",
    "    return x\n",
    "\n",
    "patch_length = 10  # Define your desired patch length\n",
    "\n",
    "# Apply patching column-wise and store results in a new 3D tensor structure\n",
    "patched_temporal_tensor = torch.stack(\n",
    "    [patching(temporal_attributes_tensor[:, i], patch_length) for i in range(temporal_attributes_tensor.shape[1])],\n",
    "    dim=0\n",
    ")\n",
    "\n",
    "# Display the shape and some elements of the patched temporal tensor\n",
    "print(\"Patched Temporal Tensor Shape:\", patched_temporal_tensor.shape)  # Should be (num_attributes, num_patches, patch_length)\n",
    "print(\"Patched Temporal Tensor:\\n\", patched_temporal_tensor[0])  # Display the patches for the first attribute (e.g., day of the week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "DatetimeIndex: 1511 entries, 2015-04-01 16:00:00 to 2021-03-31 16:00:00\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   Open    1511 non-null   float64\n",
      " 1   High    1511 non-null   float64\n",
      " 2   Low     1511 non-null   float64\n",
      " 3   Close   1511 non-null   float64\n",
      " 4   Volume  1511 non-null   int64  \n",
      "dtypes: float64(4), int64(1)\n",
      "memory usage: 70.8 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()\n",
    "\n",
    "#open : the price at which the stock has opened\n",
    "#high : the highest price of a stock\n",
    "#volume: total no of share brought in that stock\n",
    "\n",
    "# all of these are for that particular day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1511, 5)"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting with the Instance Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1781, -1.1794, -1.1788, -1.1764,  0.4678],\n",
      "        [-1.1770, -1.1798, -1.1822, -1.1839,  0.5114],\n",
      "        [-1.1826, -1.1616, -1.1811, -1.1617,  0.6332],\n",
      "        ...,\n",
      "        [ 2.2791,  2.2370,  2.2435,  2.2542, -0.3488],\n",
      "        [ 2.2251,  2.1856,  2.2296,  2.1944, -0.3793],\n",
      "        [ 2.2142,  2.2771,  2.2526,  2.2635,  0.9419]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class InstanceNormalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(InstanceNormalization, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=0, keepdim=True)\n",
    "        std = x.std(dim=0, keepdim=True)\n",
    "        return (x - mean) / std\n",
    "\n",
    "# Convert data to PyTorch tensor\n",
    "time_series = torch.tensor(df.values, dtype=torch.float32)\n",
    "\n",
    "# Apply instance normalization\n",
    "instance_norm = InstanceNormalization()\n",
    "normalized_series = instance_norm(time_series)\n",
    "\n",
    "print(normalized_series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1511, 5])"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Applying channel independence to convert multiple univariate time series data into univariate time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channel-Independent Tensor: tensor([[-1.1781],\n",
      "        [-1.1770],\n",
      "        [-1.1826],\n",
      "        ...,\n",
      "        [-0.3488],\n",
      "        [-0.3793],\n",
      "        [ 0.9419]])\n",
      "Shape of channel-independent tensor: torch.Size([7555, 1])\n"
     ]
    }
   ],
   "source": [
    "# Channel-independence\n",
    "def channel_independence(x):\n",
    "    batch_size, num_features = x.shape            #number of elements in total, number of columns\n",
    "    x=x.T\n",
    "    x = x.reshape(batch_size * num_features, 1)   # Combine time steps and features into a single dimension\n",
    "    return x\n",
    "\n",
    "#easier for trend analysis and season analysis in the time series.\n",
    "ci_series = channel_independence(normalized_series)\n",
    "\n",
    "print(\"Channel-Independent Tensor:\", ci_series)\n",
    "print(\"Shape of channel-independent tensor:\", ci_series.shape)  # Should be (1511*5, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Patching to reduce the dimensionality of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Patched Tensor: tensor([[-1.1781, -1.1770, -1.1826,  ..., -1.1640, -1.1569, -1.1576],\n",
      "        [-1.1543, -1.1592, -1.1581,  ..., -1.0611, -1.0514, -1.0348],\n",
      "        [-1.0352, -1.0373, -1.0410,  ..., -1.0555, -1.0678, -1.0442],\n",
      "        ...,\n",
      "        [-0.5621, -1.0137, -0.9568,  ...,  0.4384,  0.0021, -0.2707],\n",
      "        [ 0.6556,  0.5347, -0.3414,  ...,  0.3556,  0.2022, -0.0317],\n",
      "        [-0.0204, -0.5294, -0.2916,  ..., -0.0050,  0.1010, -0.3212]])\n",
      "Shape of patched tensor: torch.Size([755, 10])\n"
     ]
    }
   ],
   "source": [
    "def patching(x, patch_length):\n",
    "    num_elements = x.shape[0]\n",
    "    num_patches = num_elements // patch_length\n",
    "    x = x[:num_patches * patch_length]  # Ensure the tensor length is a multiple of patch_length\n",
    "    x = x.view(num_patches, patch_length)\n",
    "    return x\n",
    "\n",
    "patch_length = 10  # Example patch length\n",
    "patched_series = patching(ci_series, patch_length)\n",
    "\n",
    "print(\"Patched Tensor:\", patched_series)\n",
    "print(\"Shape of patched tensor:\", patched_series.shape)  # Should be (num_patches, patch_length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Three Encodings for Patched Time-Series Data\n",
    "\n",
    "4.1 First part of it will be to employ the new CNN encoding layer for the llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a class for the convolution token layer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvToken(nn.Module):\n",
    "    def __init__(self, patch_length, embedding_dim):\n",
    "        super(ConvToken, self).__init__()\n",
    "        # Convolutional layer with kernel size equal to patch length, producing a single embedding per patch\n",
    "        self.conv = nn.Conv1d(in_channels=1, out_channels=embedding_dim, kernel_size=patch_length, stride=1, padding=0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Add a channel dimension for Conv1d and apply convolution\n",
    "        x = x.unsqueeze(1)  # Shape: (num_patches, 1, patch_length)\n",
    "        x = self.conv(x)  # Output Shape: (num_patches, embedding_dim, 1)\n",
    "        return x.squeeze(-1)  # Final Shape: (num_patches, embedding_dim)\n",
    "\n",
    "\n",
    "# Define positional encoding class\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, num_patches, embedding_dim):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create an embedding matrix to learn positional embeddings\n",
    "        # Each position (from 0 to num_patches-1) gets a corresponding embedding of size embedding_dim\n",
    "        self.embedding = nn.Embedding(num_patches, embedding_dim)\n",
    "\n",
    "    # Forward pass method for positional encoding\n",
    "    def forward(self, x):\n",
    "        # Generate positional indices (0, 1, 2, ..., num_patches-1)\n",
    "        # These indices represent the position of each patch\n",
    "        positions = torch.arange(x.size(0), device=x.device).unsqueeze(0)\n",
    "\n",
    "        # Use the positional indices to get the corresponding positional embeddings\n",
    "        # Now we have an embedding for each position in the sequence\n",
    "        return self.embedding(positions)\n",
    "    \n",
    "\n",
    "# # Example parameters\n",
    "# patch_length = 10  # Length of each patch\n",
    "# embedding_dim = 128  # Dimension of the embedding space\n",
    "# num_patches = patched_series.shape[0]  # Number of patches in the sequence\n",
    "\n",
    "# # Initialize the ConvToken and PositionalEncoding layers\n",
    "# conv_token = ConvToken(patch_length, embedding_dim)\n",
    "# pos_encoding = PositionalEncoding(num_patches, embedding_dim)\n",
    "\n",
    "# # Generate token and positional embeddings\n",
    "# token_embeddings = conv_token(patched_series)  # Shape: (num_patches, embedding_dim)\n",
    "# pos_embeddings = pos_encoding(token_embeddings)  # Shape: (num_patches, embedding_dim)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2 Creating the temporal encoding layer to address the challenge LLMs face in processing multi-scale temporal information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[185], line 56\u001b[0m\n\u001b[0;32m     50\u001b[0m temporal_encoding_layer \u001b[38;5;241m=\u001b[39m TemporalEncoding(embedding_dim)\n\u001b[0;32m     52\u001b[0m \u001b[38;5;66;03m# Sample input with shape (5, 151, 10) representing (batch_size, num_patches, attributes)\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# patched_temporal_tensor = torch.randint(1, 32, (5, 151, 5))  # Adjusted last dim to 5 for temporal attributes\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m temporal_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtemporal_encoding_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatched_temporal_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Output\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTemporal Embedding Shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, temporal_embedding\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[185], line 31\u001b[0m, in \u001b[0;36mTemporalEncoding.forward\u001b[1;34m(self, temporal_attributes_tensor)\u001b[0m\n\u001b[0;32m     28\u001b[0m quarter \u001b[38;5;241m=\u001b[39m temporal_attributes_tensor[:, \u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mlong()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Embeddings for each attribute\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m day_of_week_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mday_of_week_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mday_of_week\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m day_of_month_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mday_of_month_embed(day_of_month)\n\u001b[0;32m     33\u001b[0m month_embed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmonth_embed(month)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:190\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:2551\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2546\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2547\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2548\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2549\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2550\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2551\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TemporalEncoding(nn.Module):\n",
    "    def __init__(self, embedding_dim):  # Removed n_patches as we're using attribute-specific ranges\n",
    "        super(TemporalEncoding, self).__init__()\n",
    "        \n",
    "        # Define trainable lookup embeddings for each temporal attribute with realistic sizes\n",
    "        self.day_of_week_embed = nn.Embedding(7, embedding_dim)  # Days of the week (0–6)\n",
    "        self.day_of_month_embed = nn.Embedding(31, embedding_dim)  # Days of month (1–31)\n",
    "        self.month_embed = nn.Embedding(12, embedding_dim)  # Months (1–12)\n",
    "        self.quarter_embed = nn.Embedding(4, embedding_dim)  # Quarters (1–4)\n",
    "        self.year_embed = nn.Embedding(101, embedding_dim)  # Years from 2000 to 2100\n",
    "\n",
    "        # Pooling method for aggregating over temporal embeddings\n",
    "        self.pooling = nn.AdaptiveMaxPool1d(1)\n",
    "\n",
    "    def forward(self, temporal_attributes_tensor):\n",
    "        # Adjust temporal_attributes_tensor to have shape (num_patches, 5) by flattening if needed\n",
    "        batch_size, num_patches, _ = temporal_attributes_tensor.size()\n",
    "        temporal_attributes_tensor = temporal_attributes_tensor.view(num_patches, -1)\n",
    "        \n",
    "        # Extract each component by indexing\n",
    "        day_of_week = temporal_attributes_tensor[:, 0].long()\n",
    "        day_of_month = temporal_attributes_tensor[:, 1].long()\n",
    "        year = (temporal_attributes_tensor[:, 2] - 2000).long()\n",
    "        month = temporal_attributes_tensor[:, 3].long()\n",
    "        quarter = temporal_attributes_tensor[:, 4].long()\n",
    "\n",
    "        # Embeddings for each attribute\n",
    "        day_of_week_embed = self.day_of_week_embed(day_of_week)\n",
    "        day_of_month_embed = self.day_of_month_embed(day_of_month)\n",
    "        month_embed = self.month_embed(month)\n",
    "        quarter_embed = self.quarter_embed(quarter)\n",
    "        year_embed = self.year_embed(year)\n",
    "\n",
    "        # Aggregate embeddings for a single temporal representation\n",
    "        temporal_embedding = (\n",
    "            day_of_week_embed + day_of_month_embed + month_embed + quarter_embed + year_embed\n",
    "        )\n",
    "\n",
    "        # Reshape back to (batch_size, num_patches, embedding_dim) for pooling\n",
    "        temporal_embedding = temporal_embedding.view(batch_size, num_patches, -1)\n",
    "        temporal_embedding = self.pooling(temporal_embedding.transpose(1, 2)).squeeze(1)\n",
    "\n",
    "        return temporal_embedding\n",
    "\n",
    "# Example usage\n",
    "embedding_dim = 128\n",
    "temporal_encoding_layer = TemporalEncoding(embedding_dim)\n",
    "\n",
    "# Sample input with shape (5, 151, 10) representing (batch_size, num_patches, attributes)\n",
    "# patched_temporal_tensor = torch.randint(1, 32, (5, 151, 5))  # Adjusted last dim to 5 for temporal attributes\n",
    "\n",
    "# Forward pass\n",
    "temporal_embedding = temporal_encoding_layer(patched_temporal_tensor)\n",
    "\n",
    "# Output\n",
    "print(\"Temporal Embedding Shape:\", temporal_embedding.shape)\n",
    "print(\"Temporal Embedding:\", temporal_embedding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3 Now preserving the parameters for the pretrained LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import copy\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Set up the access token\n",
    "HUGGING_FACE_API_KEY = \"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\"\n",
    "\n",
    "# Define model ID and file names\n",
    "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "filenames = [\n",
    "    \"LICENSE\",\n",
    "    \"README.md\",\n",
    "    \"USE_POLICY.md\",\n",
    "    \"config.json\",\n",
    "    \"generation_config.json\",\n",
    "    \"model-00001-of-00004.safetensors\",\n",
    "    \"model-00002-of-00004.safetensors\",\n",
    "    \"model-00003-of-00004.safetensors\",\n",
    "    \"model-00004-of-00004.safetensors\",\n",
    "    \"model.safetensors.index.json\",\n",
    "    \"special_tokens_map.json\",\n",
    "    \"tokenizer.json\",\n",
    "    \"tokenizer_config.json\"\n",
    "]\n",
    "\n",
    "# Download necessary model files (this is only needed if not automatically handled)\n",
    "for filename in filenames:\n",
    "    download_model_path = hf_hub_download(\n",
    "        repo_id=model_id,\n",
    "        filename=filename,\n",
    "        token=HUGGING_FACE_API_KEY\n",
    "    )\n",
    "    print(\"Downloaded:\", download_model_path)\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=HUGGING_FACE_API_KEY)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id, use_auth_token=HUGGING_FACE_API_KEY)\n",
    "\n",
    "# Set padding token if not already set\n",
    "tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n",
    "pretrained_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Custom PretrainedLLM model\n",
    "# class PretrainedLLM(nn.Module):\n",
    "#     def __init__(self, pretrained_model, num_layers, embedding_dim, prediction_dim):\n",
    "#         super(PretrainedLLM, self).__init__()\n",
    "#         # Freeze most parameters of the pre-trained model\n",
    "#         for param in pretrained_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#         # Extract specific layers from the pre-trained model\n",
    "#         # Extract specific layers from the pre-trained model for LLaMA\n",
    "#         self.transformer_blocks = nn.ModuleList([copy.deepcopy(pretrained_model.model.layers[i]) for i in range(num_layers)])\n",
    "\n",
    "\n",
    "#         # Linear output layer\n",
    "#         self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "#     def forward(self, embeddings):\n",
    "#         # Pass embeddings through selected Transformer blocks\n",
    "#         for transformer_block in self.transformer_blocks:\n",
    "#             embeddings = transformer_block(embeddings)[0]  # Only take hidden states\n",
    "\n",
    "#         # Linear output layer\n",
    "#         output = self.linear(embeddings)\n",
    "#         return output\n",
    "\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, prediction_dim):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "        \n",
    "        # Freeze pretrained model parameters\n",
    "        for param in pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Get LLaMA's hidden dimension\n",
    "        self.llama_hidden_dim = pretrained_model.config.hidden_size  # Usually 4096 for LLaMA\n",
    "        \n",
    "        # Project input embeddings to LLaMA's hidden dimension\n",
    "        self.input_projection = nn.Linear(embedding_dim, self.llama_hidden_dim)\n",
    "        \n",
    "        # Extract specific layers from the pre-trained model\n",
    "        self.transformer_blocks = nn.ModuleList([\n",
    "            copy.deepcopy(pretrained_model.model.layers[i]) \n",
    "            for i in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Project from LLaMA's hidden dimension to prediction dimension\n",
    "        self.output_projection = nn.Linear(self.llama_hidden_dim, prediction_dim)\n",
    "        \n",
    "        # Layer normalization for input and output\n",
    "        self.input_norm = nn.LayerNorm(self.llama_hidden_dim)\n",
    "        self.output_norm = nn.LayerNorm(prediction_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Project embeddings to LLaMA's hidden dimension\n",
    "        x = self.input_projection(embeddings)\n",
    "        x = self.input_norm(x)\n",
    "        \n",
    "        # Prepare attention mask (assuming no padding)\n",
    "        batch_size = x.size(0)\n",
    "        attention_mask = torch.ones(batch_size, x.size(1), device=x.device)\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.transformer_blocks:\n",
    "            # Create the expected input structure for LLaMA layers\n",
    "            transformer_outputs = block(\n",
    "                hidden_states=x,\n",
    "                attention_mask=attention_mask,\n",
    "                position_ids=None,\n",
    "                past_key_value=None,\n",
    "                output_attentions=False,\n",
    "                use_cache=False,\n",
    "            )\n",
    "            x = transformer_outputs[0]\n",
    "        \n",
    "        # Project to prediction dimension\n",
    "        x = self.output_projection(x)\n",
    "        output = self.output_norm(x)\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Example parameters\n",
    "num_layers = 12           # Number of Transformer blocks to use\n",
    "embedding_dim = 128       # Dimension of the embeddings\n",
    "num_patches = 755         # Number of patches\n",
    "prediction_dim = 128      # Dimension of prediction (output)\n",
    "\n",
    "# Initialize PretrainedLLM\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, prediction_dim)\n",
    "\n",
    "# Example input embeddings\n",
    "embeddings = torch.randn(32,num_patches, embedding_dim)\n",
    "\n",
    "# Forward pass through the custom model\n",
    "output = pretrained_llm(embeddings)\n",
    " \n",
    "# Example ground truth shifted patched data\n",
    "p_shifted = torch.randn(32,num_patches, prediction_dim)\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(output, p_shifted)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 2.398289442062378\n",
      "Epoch 2/10, Loss: 2.3941268920898438\n",
      "Epoch 3/10, Loss: 1.692588448524475\n",
      "Epoch 4/10, Loss: 1.4091089963912964\n",
      "Epoch 5/10, Loss: 1.2875704765319824\n",
      "Epoch 6/10, Loss: 1.220665454864502\n",
      "Epoch 7/10, Loss: 1.1838914155960083\n",
      "Epoch 8/10, Loss: 1.161787986755371\n",
      "Epoch 9/10, Loss: 1.1453713178634644\n",
      "Epoch 10/10, Loss: 1.1356486082077026\n",
      "Final MSE Loss: 1.1328045129776\n"
     ]
    }
   ],
   "source": [
    "'''import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "        # Freeze most parameters of the pre-trained model\n",
    "        for param in pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Extract the necessary layers from the pre-trained model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Linear output layer\n",
    "        self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "        # Apply linear output layer\n",
    "        output = self.linear(embeddings)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Example parameters\n",
    "num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "embedding_dim = 768  # Dimension of the embeddings\n",
    "num_patches = 512  # Number of patches\n",
    "prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# Initialize PretrainedLLM\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# Example input: embeddings after temporal encoding\n",
    "embeddings = torch.randn(1, num_patches, embedding_dim)  # Add a batch dimension\n",
    "\n",
    "# Forward pass\n",
    "output = pretrained_llm(embeddings)\n",
    "\n",
    "# Example ground truth shifted patched data\n",
    "p_shifted = torch.randn(1, num_patches, prediction_dim)  # Add a batch dimension\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(output, p_shifted)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())'''\n",
    "\n",
    "# Code with MSE loss of 70\n",
    "\n",
    "'''import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "        # Extract the necessary layers from the pre-trained model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Additional layers for fine-tuning\n",
    "        self.dropout = nn.Dropout(0.1)  # Add dropout for regularization\n",
    "        self.linear1 = nn.Linear(embedding_dim, prediction_dim)  # Additional linear layer\n",
    "        self.activation = nn.ReLU()  # Additional activation function\n",
    "        self.linear2 = nn.Linear(prediction_dim, prediction_dim)  # Additional linear layer\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "        # Apply additional layers\n",
    "        embeddings = self.dropout(embeddings)\n",
    "        embeddings = self.linear1(embeddings)\n",
    "        embeddings = self.activation(embeddings)\n",
    "        embeddings = self.linear2(embeddings)\n",
    "\n",
    "        return embeddings\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Example parameters\n",
    "num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "embedding_dim = 768  # Dimension of the embeddings\n",
    "num_patches = 512  # Number of patches\n",
    "prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# Initialize PretrainedLLM\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# Example input: embeddings after temporal encoding\n",
    "embeddings = torch.randn(1, num_patches, embedding_dim)  # Add a batch dimension\n",
    "\n",
    "# Forward pass\n",
    "output = pretrained_llm(embeddings)\n",
    "\n",
    "# Example ground truth shifted patched data\n",
    "p_shifted = torch.randn(1, num_patches, prediction_dim)  # Add a batch dimension\n",
    "\n",
    "# Calculate Mean Squared Error (MSE) loss\n",
    "loss_fn = nn.MSELoss()\n",
    "loss = loss_fn(output, p_shifted)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())'''\n",
    "\n",
    "#Most updated code :---\n",
    "'''import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AdamW, get_scheduler\n",
    "\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "        # Freeze most parameters of the pre-trained model\n",
    "        for param in pretrained_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Extract the necessary layers from the pre-trained model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Linear output layer\n",
    "        self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Pass the adjusted embeddings through the pre-trained Transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embeddings = transformer_block(embeddings)[0]\n",
    "\n",
    "        # Apply linear output layer\n",
    "        output = self.linear(embeddings)\n",
    "\n",
    "        return output\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Example parameters\n",
    "num_layers = 12  # Number of Transformer blocks in GPT-2\n",
    "embedding_dim = 768  # Dimension of the embeddings\n",
    "num_patches = 512  # Number of patches\n",
    "prediction_dim = 768  # Dimension of prediction (output)\n",
    "\n",
    "# Initialize PretrainedLLM\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM\n",
    "from torch.optim import AdamW\n",
    "from transformers.optimization import get_scheduler\n",
    "\n",
    "# PretrainedLLM class that builds on top of a pretrained language model (like GPT-2)\n",
    "class PretrainedLLM(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim, num_additional_layers=1, hidden_dim=512, dropout_rate=0.1, activation='relu'):\n",
    "        super(PretrainedLLM, self).__init__()\n",
    "\n",
    "        # Use a pre-trained model (like GPT-2) and extract its transformer blocks\n",
    "        self.pretrained_model = pretrained_model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Additional fully connected layers after the transformer blocks\n",
    "        # These are added to enhance the output of the pre-trained model\n",
    "        self.additional_layers = nn.ModuleList()\n",
    "        for _ in range(num_additional_layers):\n",
    "            layer = nn.Sequential(\n",
    "                nn.Linear(embedding_dim, hidden_dim),  # Linear layer to project embeddings to hidden_dim\n",
    "                nn.Dropout(dropout_rate),  # Dropout for regularization (prevent overfitting)\n",
    "                nn.ReLU() if activation == 'relu' else nn.LeakyReLU(),  # Non-linear activation function\n",
    "                nn.Linear(hidden_dim, embedding_dim)  # Linear layer to project back to embedding_dim\n",
    "            )\n",
    "            self.additional_layers.append(layer)\n",
    "\n",
    "        # Final linear layer to produce the prediction output\n",
    "        self.linear = nn.Linear(embedding_dim, prediction_dim)\n",
    "\n",
    "    def forward(self, embeddings):\n",
    "        # Pass the input embeddings through the transformer blocks\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            embeddings = transformer_block(embeddings)[0]  # Transformer block output, keeping only the embeddings\n",
    "\n",
    "        # Apply additional layers (fully connected layers) to the embeddings\n",
    "        for layer in self.additional_layers:\n",
    "            embeddings = layer(embeddings)\n",
    "\n",
    "        # Final output through the linear layer to predict the desired output\n",
    "        output = self.linear(embeddings)\n",
    "        return output\n",
    "\n",
    "# Example usage\n",
    "# Load a pre-trained GPT-2 model\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Set the number of transformer layers (12 in GPT-2), embedding dimension (768), and number of patches (512)\n",
    "num_layers = 12\n",
    "embedding_dim = 768\n",
    "num_patches = 512\n",
    "prediction_dim = 768  # The dimensionality of the output prediction\n",
    "\n",
    "# Additional settings for the fully connected layers added to the model\n",
    "num_additional_layers = 2  # Adding 2 extra layers for better predictions\n",
    "hidden_dim = 512  # The size of the hidden layers\n",
    "dropout_rate = 0.1  # Dropout rate for regularization\n",
    "activation = 'relu'  # Activation function to use (ReLU)\n",
    "\n",
    "# Initialize the PretrainedLLM model\n",
    "pretrained_llm = PretrainedLLM(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim,\n",
    "                               num_additional_layers=num_additional_layers, hidden_dim=hidden_dim,\n",
    "                               dropout_rate=dropout_rate, activation=activation)\n",
    "\n",
    "# Example input: Random tensor representing embeddings after temporal encoding (shape: [batch_size, num_patches, embedding_dim])\n",
    "embeddings = torch.randn(1, num_patches, embedding_dim)  # Adding a batch dimension (size 1)\n",
    "\n",
    "# Example ground truth: Shifted patched data for prediction task (shape: [batch_size, num_patches, prediction_dim])\n",
    "p_shifted = torch.randn(1, num_patches, prediction_dim)  # Similar batch size and patch dimension as input\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 16  # Size of the mini-batches for training\n",
    "num_epochs = 10  # Number of training iterations (epochs)\n",
    "learning_rate = 5e-5  # Learning rate for the optimizer\n",
    "\n",
    "# Define optimizer (AdamW is often used for transformer models) and a learning rate scheduler\n",
    "optimizer = AdamW(pretrained_llm.parameters(), lr=learning_rate)\n",
    "scheduler = get_scheduler(\"linear\", optimizer, num_warmup_steps=num_epochs // 10, num_training_steps=num_epochs)\n",
    "\n",
    "# Loss function: Mean Squared Error (MSE) for regression-like tasks\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Training loop to train the model over several epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass: Get model predictions from input embeddings\n",
    "    output = pretrained_llm(embeddings)\n",
    "\n",
    "    # Calculate the loss by comparing the model output with the true data (p_shifted)\n",
    "    loss = loss_fn(output, p_shifted)\n",
    "\n",
    "    # Backpropagation: Zero the gradients, compute gradients, and update weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    # Print loss for each epoch to track training progress\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "# Final evaluation of the model after training\n",
    "output = pretrained_llm(embeddings)\n",
    "final_loss = loss_fn(output, p_shifted)\n",
    "print(\"Final MSE Loss:\", final_loss.item())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5 forcasting finetuning\n",
    "+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 2.0027925968170166\n",
      "Epoch 1/10, Loss: 2.000988245010376\n",
      "Epoch 2/10, Loss: 1.9936189651489258\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 107\u001b[0m\n\u001b[0;32m    105\u001b[0m output \u001b[38;5;241m=\u001b[39m forecasting_model(embeddings)  \u001b[38;5;66;03m# Forward pass through the model\u001b[39;00m\n\u001b[0;32m    106\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, future_data)  \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Backpropagate the loss to compute gradients\u001b[39;00m\n\u001b[0;32m    108\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()  \u001b[38;5;66;03m# Update the model weights using gradients\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Output training progress\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Define the reversible instance normalization (RevIN) class\n",
    "class RevIN(nn.Module):\n",
    "    def __init__(self, num_features, affine=True):\n",
    "        super(RevIN, self).__init__()\n",
    "        self.num_features = num_features  # Number of features in the input data\n",
    "        # InstanceNorm1d applies normalization to the features across a batch\n",
    "        self.norm = nn.InstanceNorm1d(num_features, affine=affine)\n",
    "        # Learnable scale and shift parameters to adjust the normalized data\n",
    "        self.affine_scale = nn.Parameter(torch.ones(1, num_features, 1))\n",
    "        self.affine_shift = nn.Parameter(torch.zeros(1, num_features, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Transpose the input so that it's in the correct shape for normalization\n",
    "        x_transposed = x.transpose(1, 2)  # (batch_size, num_features, seq_len)\n",
    "        # Apply instance normalization to standardize the data\n",
    "        normed = self.norm(x_transposed)  # (batch_size, num_features, seq_len)\n",
    "        # Apply scaling and shifting to normalized data\n",
    "        normed = normed * self.affine_scale + self.affine_shift\n",
    "        # Transpose the data back to the original shape\n",
    "        normed = normed.transpose(1, 2)  # (batch_size, seq_len, num_features)\n",
    "\n",
    "        return normed\n",
    "\n",
    "# Main Forecasting Model that includes the RevIN and GPT-2 based transformer\n",
    "class ForecastingModel(nn.Module):\n",
    "    def __init__(self, pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim):\n",
    "        super(ForecastingModel, self).__init__()\n",
    "        # Reversible instance normalization (RevIN) to normalize input data\n",
    "        self.revin = RevIN(embedding_dim)\n",
    "\n",
    "        # Select the transformer blocks (layers) from the pretrained GPT-2 model\n",
    "        self.transformer_blocks = nn.ModuleList([pretrained_model.transformer.h[i] for i in range(num_layers)])\n",
    "\n",
    "        # Additional layers for prediction: a dropout, a linear layer, and an activation function (ReLU)\n",
    "        self.dropout = nn.Dropout(0.1)  # Dropout for regularization (prevents overfitting)\n",
    "        self.linear1 = nn.Linear(embedding_dim, embedding_dim)  # First linear transformation\n",
    "        self.activation = nn.ReLU()  # Activation function to introduce non-linearity\n",
    "        self.linear2 = nn.Linear(embedding_dim, prediction_dim)  # Second linear transformation for prediction\n",
    "\n",
    "        # Output layer to transform the prediction back to the original embedding dimension\n",
    "        self.output_linear = nn.Linear(prediction_dim, embedding_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply normalization using RevIN\n",
    "        x = self.revin(x)\n",
    "\n",
    "        # Pass the normalized data through each transformer block (pretrained GPT-2 layers)\n",
    "        for transformer_block in self.transformer_blocks:\n",
    "            x = transformer_block(x)[0]  # Extract the output from each block\n",
    "\n",
    "        # Apply the additional prediction layers (dropout -> linear -> activation -> linear)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "\n",
    "        # Final transformation and denormalize the output using RevIN\n",
    "        x = self.output_linear(x)\n",
    "        x = self.revin(x)  # Reverse the normalization to get data back in its original form\n",
    "\n",
    "        return x\n",
    "\n",
    "# Load the pretrained GPT-2 model and tokenizer for later use\n",
    "model_id = \"openai-community/gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "pretrained_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Define the model parameters\n",
    "num_layers = 12  # Use 12 transformer layers from GPT-2\n",
    "embedding_dim = 768  # The dimension of the input embeddings\n",
    "num_patches = 512  # The number of time steps (sequence length)\n",
    "prediction_dim = 768  # Prediction output dimension (same as input dimension)\n",
    "\n",
    "# Initialize the forecasting model with the GPT-2 transformer\n",
    "forecasting_model = ForecastingModel(pretrained_model, num_layers, embedding_dim, num_patches, prediction_dim)\n",
    "\n",
    "# Example input: random data to simulate temporal embeddings (e.g., stock price data)\n",
    "embeddings = torch.randn(1, num_patches, embedding_dim)  # Random input with batch size 1\n",
    "\n",
    "# Perform a forward pass through the model to get predictions\n",
    "output = forecasting_model(embeddings)\n",
    "\n",
    "# Example of ground truth future data (what we want to predict)\n",
    "future_data = torch.randn(1, num_patches, embedding_dim)  # Random target data for loss calculation\n",
    "\n",
    "# Define Mean Squared Error (MSE) as the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "# Calculate the loss between the model output (predictions) and future data\n",
    "loss = loss_fn(output, future_data)\n",
    "\n",
    "print(\"MSE Loss:\", loss.item())\n",
    "\n",
    "# Fine-tuning the model (training the model to minimize the loss)\n",
    "optimizer = torch.optim.Adam(forecasting_model.parameters(), lr=1e-4)  # Adam optimizer for gradient descent\n",
    "num_epochs = 10  # Number of times to train on the entire dataset\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    forecasting_model.train()  # Set the model to training mode\n",
    "    optimizer.zero_grad()  # Zero out gradients from the previous step\n",
    "    output = forecasting_model(embeddings)  # Forward pass through the model\n",
    "    loss = loss_fn(output, future_data)  # Calculate loss\n",
    "    loss.backard()  # Backpropagate the loss to compute gradients\n",
    "    optimizer.step()  # Update the model weights using gradients\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {loss.item()}\")  # Output training progress\n",
    "\n",
    "# Final loss after training\n",
    "print(\"Final MSE Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "                                            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
